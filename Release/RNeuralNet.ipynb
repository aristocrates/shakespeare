{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import tensorflow as tf \n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Flatten, Dropout\n",
    "from keras.layers import BatchNormalization, LSTM\n",
    "\n",
    "import string\n",
    "from preprocessor import load_sonnets\n",
    "\n",
    "sonnets = load_sonnets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = list(string.ascii_lowercase)\n",
    "#chars = list(string.ascii_letters)\n",
    "chars += [\",\", \".\", \"?\", \"!\", \":\", \";\", \"'\", \"(\", \")\", \" \", \"\\n\", \"-\"]\n",
    "\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "train_strings = []\n",
    "char_length = 41\n",
    "char_skip = 3 #20\n",
    "\n",
    "for _, s in enumerate(sonnets):\n",
    "    #sonnet = \"\\n\".join(sonnets[0]).lower()\n",
    "    sonnet = \"\\n\".join(s).lower()\n",
    "    \n",
    "    for i in range(0, len(sonnet) - char_length, char_skip):\n",
    "        train_strings.append(sonnet[i: i+char_length])\n",
    "    #break\n",
    "    \n",
    "x = np.zeros((len(train_strings), char_length - 1, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(train_strings), len(chars)), dtype=np.bool)\n",
    "\n",
    "for i, train_string in enumerate(train_strings):\n",
    "    for t, char in enumerate(train_string[:-1]):\n",
    "        x[i, t, char_indices[char]] = 1\n",
    "    y[i, char_indices[train_string[-1]]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#timesteps = 8\n",
    "model = Sequential()\n",
    "#model.add(Dense(200, input_shape=(200,)))\n",
    "model.add(LSTM(200, input_shape=(char_length - 1, len(chars))))\n",
    "model.add(Dense(200))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Dense(len(chars)))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "29325/29325 [==============================] - 19s 656us/step - loss: 2.7495 - acc: 0.2336\n",
      "Epoch 2/200\n",
      "29325/29325 [==============================] - 16s 559us/step - loss: 2.2460 - acc: 0.3403\n",
      "Epoch 3/200\n",
      "29325/29325 [==============================] - 16s 561us/step - loss: 2.0739 - acc: 0.3879\n",
      "Epoch 4/200\n",
      "29325/29325 [==============================] - 17s 565us/step - loss: 1.9701 - acc: 0.4116\n",
      "Epoch 5/200\n",
      "29325/29325 [==============================] - 17s 569us/step - loss: 1.8900 - acc: 0.4324\n",
      "Epoch 6/200\n",
      "29325/29325 [==============================] - 16s 562us/step - loss: 1.8252 - acc: 0.4495\n",
      "Epoch 7/200\n",
      "29325/29325 [==============================] - 17s 576us/step - loss: 1.7580 - acc: 0.4656\n",
      "Epoch 8/200\n",
      "29325/29325 [==============================] - 17s 564us/step - loss: 1.6967 - acc: 0.4825\n",
      "Epoch 9/200\n",
      "29325/29325 [==============================] - 17s 566us/step - loss: 1.6453 - acc: 0.4962\n",
      "Epoch 10/200\n",
      "29325/29325 [==============================] - 17s 567us/step - loss: 1.5851 - acc: 0.5098\n",
      "Epoch 11/200\n",
      "29325/29325 [==============================] - 17s 565us/step - loss: 1.5346 - acc: 0.5212\n",
      "Epoch 12/200\n",
      "29325/29325 [==============================] - 17s 566us/step - loss: 1.4798 - acc: 0.5370\n",
      "Epoch 13/200\n",
      "29325/29325 [==============================] - 17s 592us/step - loss: 1.4173 - acc: 0.5515\n",
      "Epoch 14/200\n",
      "29325/29325 [==============================] - 17s 583us/step - loss: 1.3621 - acc: 0.5643\n",
      "Epoch 15/200\n",
      "29325/29325 [==============================] - 17s 582us/step - loss: 1.2915 - acc: 0.5847\n",
      "Epoch 16/200\n",
      "29325/29325 [==============================] - 17s 580us/step - loss: 1.2327 - acc: 0.6015\n",
      "Epoch 17/200\n",
      "29325/29325 [==============================] - 17s 573us/step - loss: 1.1639 - acc: 0.6225\n",
      "Epoch 18/200\n",
      "29325/29325 [==============================] - 17s 583us/step - loss: 1.0990 - acc: 0.6435\n",
      "Epoch 19/200\n",
      "29325/29325 [==============================] - 17s 576us/step - loss: 1.0331 - acc: 0.6620\n",
      "Epoch 20/200\n",
      "29325/29325 [==============================] - 17s 568us/step - loss: 0.9707 - acc: 0.6810\n",
      "Epoch 21/200\n",
      "29325/29325 [==============================] - 17s 564us/step - loss: 0.9117 - acc: 0.6994\n",
      "Epoch 22/200\n",
      "29325/29325 [==============================] - 17s 565us/step - loss: 0.8547 - acc: 0.7175\n",
      "Epoch 23/200\n",
      "29325/29325 [==============================] - 17s 566us/step - loss: 0.7977 - acc: 0.7361\n",
      "Epoch 24/200\n",
      "29325/29325 [==============================] - 17s 567us/step - loss: 0.7515 - acc: 0.7498\n",
      "Epoch 25/200\n",
      "29325/29325 [==============================] - 17s 569us/step - loss: 0.6998 - acc: 0.7667\n",
      "Epoch 26/200\n",
      "29325/29325 [==============================] - 17s 568us/step - loss: 0.6586 - acc: 0.7799\n",
      "Epoch 27/200\n",
      "29325/29325 [==============================] - 17s 568us/step - loss: 0.6255 - acc: 0.7906\n",
      "Epoch 28/200\n",
      "29325/29325 [==============================] - 17s 568us/step - loss: 0.5634 - acc: 0.8126\n",
      "Epoch 29/200\n",
      "29325/29325 [==============================] - 17s 569us/step - loss: 0.5459 - acc: 0.8166\n",
      "Epoch 30/200\n",
      "29325/29325 [==============================] - 17s 568us/step - loss: 0.5062 - acc: 0.8307\n",
      "Epoch 31/200\n",
      "29325/29325 [==============================] - 17s 567us/step - loss: 0.4781 - acc: 0.8398\n",
      "Epoch 32/200\n",
      "29325/29325 [==============================] - 17s 568us/step - loss: 0.4607 - acc: 0.8429\n",
      "Epoch 33/200\n",
      "29325/29325 [==============================] - 17s 570us/step - loss: 0.4256 - acc: 0.8540\n",
      "Epoch 34/200\n",
      "29325/29325 [==============================] - 17s 577us/step - loss: 0.4058 - acc: 0.8643\n",
      "Epoch 35/200\n",
      "29325/29325 [==============================] - 17s 568us/step - loss: 0.3788 - acc: 0.8709\n",
      "Epoch 36/200\n",
      "29325/29325 [==============================] - 17s 569us/step - loss: 0.3656 - acc: 0.8752\n",
      "Epoch 37/200\n",
      "29325/29325 [==============================] - 17s 567us/step - loss: 0.3557 - acc: 0.8799\n",
      "Epoch 38/200\n",
      "29325/29325 [==============================] - 17s 567us/step - loss: 0.3291 - acc: 0.8894\n",
      "Epoch 39/200\n",
      "29325/29325 [==============================] - 17s 568us/step - loss: 0.3167 - acc: 0.8940\n",
      "Epoch 40/200\n",
      "29325/29325 [==============================] - 17s 569us/step - loss: 0.3122 - acc: 0.8929\n",
      "Epoch 41/200\n",
      "29325/29325 [==============================] - 17s 569us/step - loss: 0.2929 - acc: 0.9035\n",
      "Epoch 42/200\n",
      "29325/29325 [==============================] - 17s 578us/step - loss: 0.2741 - acc: 0.9073\n",
      "Epoch 43/200\n",
      "29325/29325 [==============================] - 17s 574us/step - loss: 0.2605 - acc: 0.9116\n",
      "Epoch 44/200\n",
      "29325/29325 [==============================] - 17s 570us/step - loss: 0.2755 - acc: 0.9078\n",
      "Epoch 45/200\n",
      "29325/29325 [==============================] - 17s 572us/step - loss: 0.2597 - acc: 0.9116\n",
      "Epoch 46/200\n",
      "29325/29325 [==============================] - 17s 576us/step - loss: 0.2357 - acc: 0.9208\n",
      "Epoch 47/200\n",
      "29325/29325 [==============================] - 17s 576us/step - loss: 0.2412 - acc: 0.9189\n",
      "Epoch 48/200\n",
      "29325/29325 [==============================] - 17s 569us/step - loss: 0.2392 - acc: 0.9201\n",
      "Epoch 49/200\n",
      "29325/29325 [==============================] - 17s 571us/step - loss: 0.2291 - acc: 0.9239\n",
      "Epoch 50/200\n",
      "29325/29325 [==============================] - 17s 572us/step - loss: 0.2140 - acc: 0.9290\n",
      "Epoch 51/200\n",
      "29325/29325 [==============================] - 17s 570us/step - loss: 0.2063 - acc: 0.9308\n",
      "Epoch 52/200\n",
      "29325/29325 [==============================] - 17s 582us/step - loss: 0.2063 - acc: 0.9291\n",
      "Epoch 53/200\n",
      "29325/29325 [==============================] - 17s 569us/step - loss: 0.2075 - acc: 0.9308\n",
      "Epoch 54/200\n",
      "29325/29325 [==============================] - 17s 566us/step - loss: 0.2001 - acc: 0.9317\n",
      "Epoch 55/200\n",
      "29325/29325 [==============================] - 17s 565us/step - loss: 0.2102 - acc: 0.9286\n",
      "Epoch 56/200\n",
      "29325/29325 [==============================] - 17s 569us/step - loss: 0.1929 - acc: 0.9352\n",
      "Epoch 57/200\n",
      "29325/29325 [==============================] - 17s 567us/step - loss: 0.1772 - acc: 0.9402\n",
      "Epoch 58/200\n",
      "29325/29325 [==============================] - 17s 568us/step - loss: 0.1769 - acc: 0.9384\n",
      "Epoch 59/200\n",
      "29325/29325 [==============================] - 17s 567us/step - loss: 0.2047 - acc: 0.9305\n",
      "Epoch 60/200\n",
      "29325/29325 [==============================] - 17s 567us/step - loss: 0.1792 - acc: 0.9395\n",
      "Epoch 61/200\n",
      "29325/29325 [==============================] - 17s 567us/step - loss: 0.1701 - acc: 0.9440\n",
      "Epoch 62/200\n",
      "29325/29325 [==============================] - 17s 568us/step - loss: 0.1723 - acc: 0.9423\n",
      "Epoch 63/200\n",
      "29325/29325 [==============================] - 17s 568us/step - loss: 0.1760 - acc: 0.9415\n",
      "Epoch 64/200\n",
      "29325/29325 [==============================] - 17s 565us/step - loss: 0.1672 - acc: 0.9432\n",
      "Epoch 65/200\n",
      "29325/29325 [==============================] - 17s 568us/step - loss: 0.1696 - acc: 0.9439\n",
      "Epoch 66/200\n",
      "29325/29325 [==============================] - 17s 569us/step - loss: 0.1530 - acc: 0.9480\n",
      "Epoch 67/200\n",
      "29325/29325 [==============================] - 17s 568us/step - loss: 0.1598 - acc: 0.9460\n",
      "Epoch 68/200\n",
      "29325/29325 [==============================] - 17s 567us/step - loss: 0.1625 - acc: 0.9448\n",
      "Epoch 69/200\n",
      "29325/29325 [==============================] - 17s 567us/step - loss: 0.1662 - acc: 0.9439\n",
      "Epoch 70/200\n",
      "29325/29325 [==============================] - 17s 566us/step - loss: 0.1491 - acc: 0.9505\n",
      "Epoch 71/200\n",
      "29325/29325 [==============================] - 17s 568us/step - loss: 0.1527 - acc: 0.9485\n",
      "Epoch 72/200\n",
      "29325/29325 [==============================] - 17s 565us/step - loss: 0.1542 - acc: 0.9494\n",
      "Epoch 73/200\n",
      "29325/29325 [==============================] - 17s 569us/step - loss: 0.1503 - acc: 0.9498\n",
      "Epoch 74/200\n",
      "29325/29325 [==============================] - 17s 569us/step - loss: 0.1422 - acc: 0.9525\n",
      "Epoch 75/200\n",
      "29325/29325 [==============================] - 17s 569us/step - loss: 0.1400 - acc: 0.9521\n",
      "Epoch 76/200\n",
      "29325/29325 [==============================] - 17s 569us/step - loss: 0.1401 - acc: 0.9526\n",
      "Epoch 77/200\n",
      "29325/29325 [==============================] - 17s 570us/step - loss: 0.1492 - acc: 0.9498\n",
      "Epoch 78/200\n",
      "29325/29325 [==============================] - 17s 570us/step - loss: 0.1422 - acc: 0.9517\n",
      "Epoch 79/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29325/29325 [==============================] - 16s 560us/step - loss: 0.1341 - acc: 0.9565\n",
      "Epoch 80/200\n",
      "29325/29325 [==============================] - 16s 557us/step - loss: 0.1339 - acc: 0.9542\n",
      "Epoch 81/200\n",
      "29325/29325 [==============================] - 16s 557us/step - loss: 0.1410 - acc: 0.9526\n",
      "Epoch 82/200\n",
      "29325/29325 [==============================] - 16s 557us/step - loss: 0.1537 - acc: 0.9484\n",
      "Epoch 83/200\n",
      "29325/29325 [==============================] - 16s 558us/step - loss: 0.1372 - acc: 0.9527\n",
      "Epoch 84/200\n",
      "29325/29325 [==============================] - 16s 557us/step - loss: 0.1227 - acc: 0.9596\n",
      "Epoch 85/200\n",
      "29325/29325 [==============================] - 16s 558us/step - loss: 0.1255 - acc: 0.9593\n",
      "Epoch 86/200\n",
      "29325/29325 [==============================] - 16s 557us/step - loss: 0.1368 - acc: 0.9528\n",
      "Epoch 87/200\n",
      "29325/29325 [==============================] - 16s 557us/step - loss: 0.1422 - acc: 0.9533\n",
      "Epoch 88/200\n",
      "29325/29325 [==============================] - 16s 557us/step - loss: 0.1271 - acc: 0.9567\n",
      "Epoch 89/200\n",
      "29325/29325 [==============================] - 16s 559us/step - loss: 0.1175 - acc: 0.9607\n",
      "Epoch 90/200\n",
      "29325/29325 [==============================] - 16s 558us/step - loss: 0.1290 - acc: 0.9562\n",
      "Epoch 91/200\n",
      "29325/29325 [==============================] - 16s 558us/step - loss: 0.1197 - acc: 0.9596\n",
      "Epoch 92/200\n",
      "29325/29325 [==============================] - 16s 558us/step - loss: 0.1290 - acc: 0.9569\n",
      "Epoch 93/200\n",
      "29325/29325 [==============================] - 16s 558us/step - loss: 0.1232 - acc: 0.9595\n",
      "Epoch 94/200\n",
      "29325/29325 [==============================] - 16s 559us/step - loss: 0.1441 - acc: 0.9521\n",
      "Epoch 95/200\n",
      "29325/29325 [==============================] - 16s 559us/step - loss: 0.1296 - acc: 0.9566\n",
      "Epoch 96/200\n",
      "29325/29325 [==============================] - 16s 559us/step - loss: 0.1279 - acc: 0.9568\n",
      "Epoch 97/200\n",
      "29325/29325 [==============================] - 16s 559us/step - loss: 0.1270 - acc: 0.9578\n",
      "Epoch 98/200\n",
      "29325/29325 [==============================] - 16s 560us/step - loss: 0.1167 - acc: 0.9617\n",
      "Epoch 99/200\n",
      "29325/29325 [==============================] - 16s 560us/step - loss: 0.1196 - acc: 0.9599\n",
      "Epoch 100/200\n",
      "29325/29325 [==============================] - 17s 567us/step - loss: 0.1014 - acc: 0.9669\n",
      "Epoch 101/200\n",
      "29325/29325 [==============================] - 16s 559us/step - loss: 0.1053 - acc: 0.9651\n",
      "Epoch 102/200\n",
      "29325/29325 [==============================] - 16s 559us/step - loss: 0.1227 - acc: 0.9591\n",
      "Epoch 103/200\n",
      "29325/29325 [==============================] - 16s 559us/step - loss: 0.1132 - acc: 0.9612\n",
      "Epoch 104/200\n",
      "29325/29325 [==============================] - 16s 559us/step - loss: 0.1101 - acc: 0.9628\n",
      "Epoch 105/200\n",
      "29325/29325 [==============================] - 16s 559us/step - loss: 0.1237 - acc: 0.9585\n",
      "Epoch 106/200\n",
      "29325/29325 [==============================] - 16s 559us/step - loss: 0.1137 - acc: 0.9613\n",
      "Epoch 107/200\n",
      "29325/29325 [==============================] - 16s 560us/step - loss: 0.1045 - acc: 0.9639\n",
      "Epoch 108/200\n",
      "29325/29325 [==============================] - 16s 560us/step - loss: 0.1102 - acc: 0.9631\n",
      "Epoch 109/200\n",
      "29325/29325 [==============================] - 16s 560us/step - loss: 0.1206 - acc: 0.9590\n",
      "Epoch 110/200\n",
      "29325/29325 [==============================] - 16s 559us/step - loss: 0.1097 - acc: 0.9627\n",
      "Epoch 111/200\n",
      "29325/29325 [==============================] - 16s 558us/step - loss: 0.1136 - acc: 0.9636\n",
      "Epoch 112/200\n",
      "29325/29325 [==============================] - 16s 561us/step - loss: 0.0987 - acc: 0.9669\n",
      "Epoch 113/200\n",
      "29325/29325 [==============================] - 16s 560us/step - loss: 0.1041 - acc: 0.9656\n",
      "Epoch 114/200\n",
      "29325/29325 [==============================] - 16s 561us/step - loss: 0.1188 - acc: 0.9612\n",
      "Epoch 115/200\n",
      "29325/29325 [==============================] - 16s 560us/step - loss: 0.1063 - acc: 0.9641\n",
      "Epoch 116/200\n",
      "29325/29325 [==============================] - 17s 563us/step - loss: 0.1087 - acc: 0.9637\n",
      "Epoch 117/200\n",
      "29325/29325 [==============================] - 16s 560us/step - loss: 0.1019 - acc: 0.9662\n",
      "Epoch 118/200\n",
      "29325/29325 [==============================] - 16s 560us/step - loss: 0.0976 - acc: 0.9672\n",
      "Epoch 119/200\n",
      "29325/29325 [==============================] - 16s 562us/step - loss: 0.1046 - acc: 0.9659\n",
      "Epoch 120/200\n",
      "29325/29325 [==============================] - 16s 561us/step - loss: 0.0966 - acc: 0.9668\n",
      "Epoch 121/200\n",
      "29325/29325 [==============================] - 16s 562us/step - loss: 0.0898 - acc: 0.9696\n",
      "Epoch 122/200\n",
      "29325/29325 [==============================] - 16s 561us/step - loss: 0.1142 - acc: 0.9631\n",
      "Epoch 123/200\n",
      "29325/29325 [==============================] - 16s 560us/step - loss: 0.1127 - acc: 0.9627\n",
      "Epoch 124/200\n",
      "29325/29325 [==============================] - 16s 560us/step - loss: 0.0976 - acc: 0.9678\n",
      "Epoch 125/200\n",
      "29325/29325 [==============================] - 16s 562us/step - loss: 0.0940 - acc: 0.9680\n",
      "Epoch 126/200\n",
      "29325/29325 [==============================] - 16s 561us/step - loss: 0.0921 - acc: 0.9690\n",
      "Epoch 127/200\n",
      "29325/29325 [==============================] - 16s 561us/step - loss: 0.1145 - acc: 0.9622\n",
      "Epoch 128/200\n",
      "29325/29325 [==============================] - 16s 562us/step - loss: 0.1015 - acc: 0.9653\n",
      "Epoch 129/200\n",
      "29325/29325 [==============================] - 16s 562us/step - loss: 0.0934 - acc: 0.9694\n",
      "Epoch 130/200\n",
      "29325/29325 [==============================] - 16s 562us/step - loss: 0.0959 - acc: 0.9679\n",
      "Epoch 131/200\n",
      "29325/29325 [==============================] - 16s 562us/step - loss: 0.0878 - acc: 0.9701\n",
      "Epoch 132/200\n",
      "29325/29325 [==============================] - 16s 561us/step - loss: 0.1034 - acc: 0.9656\n",
      "Epoch 133/200\n",
      "29325/29325 [==============================] - 16s 561us/step - loss: 0.0940 - acc: 0.9685\n",
      "Epoch 134/200\n",
      "29325/29325 [==============================] - 16s 561us/step - loss: 0.0985 - acc: 0.9682\n",
      "Epoch 135/200\n",
      "29325/29325 [==============================] - 16s 561us/step - loss: 0.0877 - acc: 0.9720\n",
      "Epoch 136/200\n",
      "29325/29325 [==============================] - 16s 562us/step - loss: 0.0978 - acc: 0.9670\n",
      "Epoch 137/200\n",
      "29325/29325 [==============================] - 16s 560us/step - loss: 0.1028 - acc: 0.9659\n",
      "Epoch 138/200\n",
      "29325/29325 [==============================] - 17s 563us/step - loss: 0.0916 - acc: 0.9699\n",
      "Epoch 139/200\n",
      "29325/29325 [==============================] - 16s 562us/step - loss: 0.0938 - acc: 0.9679\n",
      "Epoch 140/200\n",
      "29325/29325 [==============================] - 16s 561us/step - loss: 0.0885 - acc: 0.9710\n",
      "Epoch 141/200\n",
      "29325/29325 [==============================] - 16s 561us/step - loss: 0.0909 - acc: 0.9693\n",
      "Epoch 142/200\n",
      "29325/29325 [==============================] - 16s 562us/step - loss: 0.0940 - acc: 0.9697\n",
      "Epoch 143/200\n",
      "29325/29325 [==============================] - 17s 564us/step - loss: 0.0886 - acc: 0.9708\n",
      "Epoch 144/200\n",
      "29325/29325 [==============================] - 16s 561us/step - loss: 0.0888 - acc: 0.9696\n",
      "Epoch 145/200\n",
      "29325/29325 [==============================] - 16s 560us/step - loss: 0.0927 - acc: 0.9693\n",
      "Epoch 146/200\n",
      "29325/29325 [==============================] - 16s 561us/step - loss: 0.1057 - acc: 0.9650\n",
      "Epoch 147/200\n",
      "29325/29325 [==============================] - 17s 563us/step - loss: 0.0852 - acc: 0.9714\n",
      "Epoch 148/200\n",
      "29325/29325 [==============================] - 16s 561us/step - loss: 0.0826 - acc: 0.9721\n",
      "Epoch 149/200\n",
      "29325/29325 [==============================] - 16s 560us/step - loss: 0.0797 - acc: 0.9738\n",
      "Epoch 150/200\n",
      "29325/29325 [==============================] - 17s 565us/step - loss: 0.0943 - acc: 0.9684\n",
      "Epoch 151/200\n",
      "29325/29325 [==============================] - 17s 563us/step - loss: 0.0978 - acc: 0.9665\n",
      "Epoch 152/200\n",
      "29325/29325 [==============================] - 17s 565us/step - loss: 0.0908 - acc: 0.9699\n",
      "Epoch 153/200\n",
      "29325/29325 [==============================] - 16s 563us/step - loss: 0.0828 - acc: 0.9718\n",
      "Epoch 154/200\n",
      "29325/29325 [==============================] - 16s 562us/step - loss: 0.0815 - acc: 0.9741\n",
      "Epoch 155/200\n",
      "29325/29325 [==============================] - 16s 562us/step - loss: 0.0829 - acc: 0.9724\n",
      "Epoch 156/200\n",
      "29325/29325 [==============================] - 17s 563us/step - loss: 0.0786 - acc: 0.9733\n",
      "Epoch 157/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29325/29325 [==============================] - 16s 557us/step - loss: 0.0893 - acc: 0.9702\n",
      "Epoch 158/200\n",
      "29325/29325 [==============================] - 16s 558us/step - loss: 0.0847 - acc: 0.9722\n",
      "Epoch 159/200\n",
      "29325/29325 [==============================] - 16s 557us/step - loss: 0.0865 - acc: 0.9708\n",
      "Epoch 160/200\n",
      "29325/29325 [==============================] - 16s 557us/step - loss: 0.0895 - acc: 0.9714\n",
      "Epoch 161/200\n",
      "29325/29325 [==============================] - 16s 557us/step - loss: 0.0771 - acc: 0.9744\n",
      "Epoch 162/200\n",
      "29325/29325 [==============================] - 16s 558us/step - loss: 0.0742 - acc: 0.9749\n",
      "Epoch 163/200\n",
      "29325/29325 [==============================] - 16s 557us/step - loss: 0.0902 - acc: 0.9705\n",
      "Epoch 164/200\n",
      "29325/29325 [==============================] - 16s 558us/step - loss: 0.0804 - acc: 0.9732\n",
      "Epoch 165/200\n",
      "29325/29325 [==============================] - 16s 558us/step - loss: 0.0877 - acc: 0.9711\n",
      "Epoch 166/200\n",
      "29325/29325 [==============================] - 16s 557us/step - loss: 0.0830 - acc: 0.9725\n",
      "Epoch 167/200\n",
      "29325/29325 [==============================] - 16s 558us/step - loss: 0.0809 - acc: 0.9716\n",
      "Epoch 168/200\n",
      "29325/29325 [==============================] - 16s 558us/step - loss: 0.0769 - acc: 0.9733\n",
      "Epoch 169/200\n",
      "29325/29325 [==============================] - 16s 558us/step - loss: 0.0789 - acc: 0.9748\n",
      "Epoch 170/200\n",
      "29325/29325 [==============================] - 16s 560us/step - loss: 0.0848 - acc: 0.9721\n",
      "Epoch 171/200\n",
      "29325/29325 [==============================] - 16s 558us/step - loss: 0.0830 - acc: 0.9722\n",
      "Epoch 172/200\n",
      "29325/29325 [==============================] - 16s 558us/step - loss: 0.0806 - acc: 0.9737\n",
      "Epoch 173/200\n",
      "29325/29325 [==============================] - 16s 558us/step - loss: 0.0811 - acc: 0.9736\n",
      "Epoch 174/200\n",
      "29325/29325 [==============================] - 16s 558us/step - loss: 0.0735 - acc: 0.9752\n",
      "Epoch 175/200\n",
      "29325/29325 [==============================] - 16s 558us/step - loss: 0.0702 - acc: 0.9773\n",
      "Epoch 176/200\n",
      "29325/29325 [==============================] - 16s 559us/step - loss: 0.0773 - acc: 0.9742\n",
      "Epoch 177/200\n",
      "29325/29325 [==============================] - 16s 559us/step - loss: 0.0819 - acc: 0.9730\n",
      "Epoch 178/200\n",
      "29325/29325 [==============================] - 16s 559us/step - loss: 0.0827 - acc: 0.9732\n",
      "Epoch 179/200\n",
      "29325/29325 [==============================] - 16s 560us/step - loss: 0.0826 - acc: 0.9723\n",
      "Epoch 180/200\n",
      "29325/29325 [==============================] - 16s 561us/step - loss: 0.0664 - acc: 0.9789\n",
      "Epoch 181/200\n",
      "29325/29325 [==============================] - 16s 560us/step - loss: 0.0786 - acc: 0.9732\n",
      "Epoch 182/200\n",
      "29325/29325 [==============================] - 16s 559us/step - loss: 0.0753 - acc: 0.9746\n",
      "Epoch 183/200\n",
      "29325/29325 [==============================] - 16s 559us/step - loss: 0.0853 - acc: 0.9718\n",
      "Epoch 184/200\n",
      "29325/29325 [==============================] - 16s 559us/step - loss: 0.0853 - acc: 0.9729\n",
      "Epoch 185/200\n",
      "29325/29325 [==============================] - 16s 559us/step - loss: 0.0767 - acc: 0.9737\n",
      "Epoch 186/200\n",
      "29325/29325 [==============================] - 16s 560us/step - loss: 0.0647 - acc: 0.9778\n",
      "Epoch 187/200\n",
      "29325/29325 [==============================] - 16s 561us/step - loss: 0.0760 - acc: 0.9760\n",
      "Epoch 188/200\n",
      "29325/29325 [==============================] - 16s 561us/step - loss: 0.0853 - acc: 0.9717\n",
      "Epoch 189/200\n",
      "29325/29325 [==============================] - 16s 561us/step - loss: 0.0856 - acc: 0.9708\n",
      "Epoch 190/200\n",
      "29325/29325 [==============================] - 16s 559us/step - loss: 0.0608 - acc: 0.9796\n",
      "Epoch 191/200\n",
      "29325/29325 [==============================] - 16s 561us/step - loss: 0.0690 - acc: 0.9784\n",
      "Epoch 192/200\n",
      "29325/29325 [==============================] - 16s 560us/step - loss: 0.0697 - acc: 0.9770\n",
      "Epoch 193/200\n",
      "29325/29325 [==============================] - 16s 561us/step - loss: 0.0673 - acc: 0.9779\n",
      "Epoch 194/200\n",
      "29325/29325 [==============================] - 16s 560us/step - loss: 0.0739 - acc: 0.9755\n",
      "Epoch 195/200\n",
      "29325/29325 [==============================] - 16s 560us/step - loss: 0.0772 - acc: 0.9736\n",
      "Epoch 196/200\n",
      "29325/29325 [==============================] - 16s 560us/step - loss: 0.0712 - acc: 0.9768\n",
      "Epoch 197/200\n",
      "29325/29325 [==============================] - 16s 561us/step - loss: 0.0742 - acc: 0.9755\n",
      "Epoch 198/200\n",
      "29325/29325 [==============================] - 16s 561us/step - loss: 0.0663 - acc: 0.9779\n",
      "Epoch 199/200\n",
      "29325/29325 [==============================] - 16s 561us/step - loss: 0.0678 - acc: 0.9787\n",
      "Epoch 200/200\n",
      "29325/29325 [==============================] - 16s 560us/step - loss: 0.0755 - acc: 0.9740\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f61d7dc0908>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model.summary()\n",
    "model.fit(x, y, batch_size=64, epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nmeyer/Documents/cs155/project3/tensorflow/lib64/python3.6/site-packages/ipykernel_launcher.py:4: RuntimeWarning: divide by zero encountered in log\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shall i compare thee to a summer's day?\n",
      "the rogon virsume on the world the king,\n",
      "that pressuch me greterest beauty this tell,\n",
      "the pun the foun to me, then thou hast the sweet,\n",
      "which gailly side own is her breas noun groen,\n",
      "while pind, but gines of my sumfaring that the fairen thee,\n",
      "one plaise they me dave sweet of worth.\n",
      "  and i his some place, and all my eart did mop\n",
      "thy from you, in proud soun, thy me,\n",
      "  to gave thee my duth my sinf love's reish,\n",
      "and notherd what heaven sweet beloed,\n",
      "or whet i me, thou most proides should her ell;ensuring,\n",
      "  th's seef is olt, and have what i thine\n",
      "the povers and the pisse childle be but dume,\n",
      "what the from him negion confect, and cansred:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def generate_poem(seed = \"shall i compare thee to a summer's day?\\n\", lines=14):\n",
    "    generated = \"\" + seed\n",
    "\n",
    "    # Generates 14 lines of poem\n",
    "    \n",
    "    for i in range(lines):\n",
    "        while True:\n",
    "            x_pred = np.zeros((1, char_length - 1, len(chars)))\n",
    "            for t, char in enumerate(seed):\n",
    "                x_pred[0, t, char_indices[char]] = 1.\n",
    "            preds = model.predict(x_pred, verbose=0)[0]\n",
    "        \n",
    "            next_index = sample(preds, temperature=0.5)\n",
    "            next_char = indices_char[next_index]\n",
    "            seed = seed[1:] + next_char\n",
    "        \n",
    "            generated += next_char\n",
    "            if next_char == \"\\n\":\n",
    "                #print(generated)\n",
    "                #generated = \"\"\n",
    "                break\n",
    "            #generated += next_char\n",
    "\n",
    "    return generated\n",
    "    \n",
    "print(generate_poem())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nmeyer/Documents/cs155/project3/tensorflow/lib64/python3.6/site-packages/ipykernel_launcher.py:4: RuntimeWarning: divide by zero encountered in log\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shall i compare thee to a summer's day?\n",
      "the rogon virsume on the world that i shall faye,\n",
      "my veriffire being thee thy pouls priss,\n",
      "but my my self i'll from the fines of lanters thee,\n",
      "o then grown werk reinl the keen gaze,\n",
      "when i haspeading their pup i not revell.\n",
      "for when i confect an thee be coce.\n",
      "  leat thy would mass diver which thee thy beauty,\n",
      "force thinks my brit find but i may the purse,\n",
      "my poor full and id livion dand a fir,\n",
      "  as first my sin graise this, my love not dumm,\n",
      "  then thou be rich lie, by bain, this freen me,\n",
      "when i have villig the fin's of firser slace,\n",
      "but and her my for my summer's dunterest,\n",
      "which in the boses stand all such a change fre,\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#seed = \"shall i compare thee to a summer's day?\\nthou art more lovely\"\n",
    "seed = \"shall i compare thee to a summer's day?\\n\"\n",
    "generated = \"\" + seed\n",
    "\n",
    "# Generates 14 lines of poem (More efficient)\n",
    "\n",
    "x_pred = np.zeros((1, char_length - 1, len(chars)))\n",
    "for t, char in enumerate(seed):\n",
    "    x_pred[0, t, char_indices[char]] = 1.\n",
    "preds = model.predict(x_pred, verbose=0)[0]\n",
    "\n",
    "for i in range(14):\n",
    "    while True:\n",
    "        next_index = sample(preds, temperature=0.5)\n",
    "        next_char = indices_char[next_index]\n",
    "        \n",
    "        x_pred[0, :-1, :] = x_pred[:, 1:, :]\n",
    "        x_pred[0, -1, :] = np.zeros(len(chars))\n",
    "        x_pred[0, -1, char_indices[next_char]] = 1.\n",
    "        preds = model.predict(x_pred, verbose=0)[0]\n",
    "        \n",
    "        generated += next_char\n",
    "        if next_char == \"\\n\":\n",
    "            #print(generated)\n",
    "            #generated = \"\"\n",
    "            break\n",
    "        #generated += next_char\n",
    "        \n",
    "\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29325, 38)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(y.shape)\n",
    "#print(generated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if training it for longer makes it better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "29325/29325 [==============================] - 18s 623us/step - loss: 0.0784 - acc: 0.9731\n",
      "Epoch 2/200\n",
      "29325/29325 [==============================] - 18s 601us/step - loss: 0.0796 - acc: 0.9735\n",
      "Epoch 3/200\n",
      "29325/29325 [==============================] - 17s 571us/step - loss: 0.0696 - acc: 0.9777\n",
      "Epoch 4/200\n",
      "29325/29325 [==============================] - 17s 565us/step - loss: 0.0701 - acc: 0.9767\n",
      "Epoch 5/200\n",
      "29325/29325 [==============================] - 17s 575us/step - loss: 0.0638 - acc: 0.9791\n",
      "Epoch 6/200\n",
      "29325/29325 [==============================] - 17s 566us/step - loss: 0.0627 - acc: 0.9793\n",
      "Epoch 7/200\n",
      "29325/29325 [==============================] - 17s 567us/step - loss: 0.0647 - acc: 0.9787\n",
      "Epoch 8/200\n",
      "29325/29325 [==============================] - 17s 568us/step - loss: 0.0710 - acc: 0.9759\n",
      "Epoch 9/200\n",
      "29325/29325 [==============================] - 16s 561us/step - loss: 0.0711 - acc: 0.9773\n",
      "Epoch 10/200\n",
      "29325/29325 [==============================] - 16s 562us/step - loss: 0.0723 - acc: 0.9749\n",
      "Epoch 11/200\n",
      "29325/29325 [==============================] - 16s 560us/step - loss: 0.0700 - acc: 0.9760\n",
      "Epoch 12/200\n",
      "29325/29325 [==============================] - 16s 560us/step - loss: 0.0713 - acc: 0.9763\n",
      "Epoch 13/200\n",
      "29325/29325 [==============================] - 16s 560us/step - loss: 0.0634 - acc: 0.9792\n",
      "Epoch 14/200\n",
      "29325/29325 [==============================] - 16s 561us/step - loss: 0.0722 - acc: 0.9764\n",
      "Epoch 15/200\n",
      "29325/29325 [==============================] - 17s 567us/step - loss: 0.0728 - acc: 0.9763\n",
      "Epoch 16/200\n",
      "29325/29325 [==============================] - 16s 562us/step - loss: 0.0705 - acc: 0.9762\n",
      "Epoch 17/200\n",
      "29325/29325 [==============================] - 17s 567us/step - loss: 0.0608 - acc: 0.9797\n",
      "Epoch 18/200\n",
      "29325/29325 [==============================] - 17s 565us/step - loss: 0.0559 - acc: 0.9814\n",
      "Epoch 19/200\n",
      "29325/29325 [==============================] - 17s 588us/step - loss: 0.0583 - acc: 0.9801\n",
      "Epoch 20/200\n",
      "29325/29325 [==============================] - 18s 608us/step - loss: 0.0699 - acc: 0.9774\n",
      "Epoch 21/200\n",
      "29325/29325 [==============================] - 17s 565us/step - loss: 0.0713 - acc: 0.9760\n",
      "Epoch 22/200\n",
      "29325/29325 [==============================] - 17s 574us/step - loss: 0.0746 - acc: 0.9755\n",
      "Epoch 23/200\n",
      "29325/29325 [==============================] - 17s 566us/step - loss: 0.0684 - acc: 0.9770\n",
      "Epoch 24/200\n",
      "29325/29325 [==============================] - 16s 547us/step - loss: 0.0673 - acc: 0.9785\n",
      "Epoch 25/200\n",
      "29325/29325 [==============================] - 16s 545us/step - loss: 0.0620 - acc: 0.9800\n",
      "Epoch 26/200\n",
      "29325/29325 [==============================] - 16s 549us/step - loss: 0.0570 - acc: 0.9811\n",
      "Epoch 27/200\n",
      "29325/29325 [==============================] - 17s 593us/step - loss: 0.0722 - acc: 0.9758\n",
      "Epoch 28/200\n",
      "29325/29325 [==============================] - 17s 577us/step - loss: 0.0668 - acc: 0.9788\n",
      "Epoch 29/200\n",
      "29325/29325 [==============================] - 17s 577us/step - loss: 0.0634 - acc: 0.9791\n",
      "Epoch 30/200\n",
      "29325/29325 [==============================] - 18s 600us/step - loss: 0.0609 - acc: 0.9809\n",
      "Epoch 31/200\n",
      "29325/29325 [==============================] - 16s 553us/step - loss: 0.0613 - acc: 0.9796\n",
      "Epoch 32/200\n",
      "29325/29325 [==============================] - 16s 559us/step - loss: 0.0624 - acc: 0.9790\n",
      "Epoch 33/200\n",
      "29325/29325 [==============================] - 16s 546us/step - loss: 0.0685 - acc: 0.9773\n",
      "Epoch 34/200\n",
      "29325/29325 [==============================] - 17s 564us/step - loss: 0.0710 - acc: 0.9758\n",
      "Epoch 35/200\n",
      "29325/29325 [==============================] - 16s 560us/step - loss: 0.0611 - acc: 0.9805\n",
      "Epoch 36/200\n",
      "29325/29325 [==============================] - 16s 555us/step - loss: 0.0676 - acc: 0.9781\n",
      "Epoch 37/200\n",
      "29325/29325 [==============================] - 17s 583us/step - loss: 0.0586 - acc: 0.9800\n",
      "Epoch 38/200\n",
      "29325/29325 [==============================] - 16s 547us/step - loss: 0.0580 - acc: 0.9816\n",
      "Epoch 39/200\n",
      "29325/29325 [==============================] - 17s 566us/step - loss: 0.0599 - acc: 0.9798\n",
      "Epoch 40/200\n",
      "29325/29325 [==============================] - 17s 578us/step - loss: 0.0563 - acc: 0.9822\n",
      "Epoch 41/200\n",
      "29325/29325 [==============================] - 17s 586us/step - loss: 0.0658 - acc: 0.9781\n",
      "Epoch 42/200\n",
      "29325/29325 [==============================] - 17s 578us/step - loss: 0.0641 - acc: 0.9792\n",
      "Epoch 43/200\n",
      "29325/29325 [==============================] - 17s 568us/step - loss: 0.0701 - acc: 0.9761\n",
      "Epoch 44/200\n",
      "29325/29325 [==============================] - 17s 567us/step - loss: 0.0663 - acc: 0.9788\n",
      "Epoch 45/200\n",
      "29325/29325 [==============================] - 17s 568us/step - loss: 0.0604 - acc: 0.9802\n",
      "Epoch 46/200\n",
      "29325/29325 [==============================] - 17s 565us/step - loss: 0.0550 - acc: 0.9811\n",
      "Epoch 47/200\n",
      "29325/29325 [==============================] - 17s 564us/step - loss: 0.0565 - acc: 0.9815\n",
      "Epoch 48/200\n",
      "29325/29325 [==============================] - 17s 568us/step - loss: 0.0620 - acc: 0.9804\n",
      "Epoch 49/200\n",
      "29325/29325 [==============================] - 17s 567us/step - loss: 0.0572 - acc: 0.9812\n",
      "Epoch 50/200\n",
      "29325/29325 [==============================] - 17s 566us/step - loss: 0.0584 - acc: 0.9801\n",
      "Epoch 51/200\n",
      "29325/29325 [==============================] - 17s 569us/step - loss: 0.0597 - acc: 0.9798\n",
      "Epoch 52/200\n",
      "29325/29325 [==============================] - 17s 570us/step - loss: 0.0624 - acc: 0.9785\n",
      "Epoch 53/200\n",
      "29325/29325 [==============================] - 17s 571us/step - loss: 0.0605 - acc: 0.9803\n",
      "Epoch 54/200\n",
      "29325/29325 [==============================] - 17s 568us/step - loss: 0.0605 - acc: 0.9798\n",
      "Epoch 55/200\n",
      "29325/29325 [==============================] - 17s 572us/step - loss: 0.0606 - acc: 0.9802\n",
      "Epoch 56/200\n",
      "29325/29325 [==============================] - 17s 595us/step - loss: 0.0618 - acc: 0.9799\n",
      "Epoch 57/200\n",
      "29325/29325 [==============================] - 18s 601us/step - loss: 0.0574 - acc: 0.9809\n",
      "Epoch 58/200\n",
      "29325/29325 [==============================] - 17s 575us/step - loss: 0.0513 - acc: 0.9819\n",
      "Epoch 59/200\n",
      "29325/29325 [==============================] - 17s 596us/step - loss: 0.0552 - acc: 0.9811\n",
      "Epoch 60/200\n",
      "29325/29325 [==============================] - 17s 572us/step - loss: 0.0657 - acc: 0.9780\n",
      "Epoch 61/200\n",
      "29325/29325 [==============================] - 17s 577us/step - loss: 0.0638 - acc: 0.9785\n",
      "Epoch 62/200\n",
      "29325/29325 [==============================] - 16s 555us/step - loss: 0.0516 - acc: 0.9829\n",
      "Epoch 63/200\n",
      "29325/29325 [==============================] - 16s 562us/step - loss: 0.0466 - acc: 0.9841\n",
      "Epoch 64/200\n",
      "29325/29325 [==============================] - 17s 564us/step - loss: 0.0592 - acc: 0.9803\n",
      "Epoch 65/200\n",
      "29325/29325 [==============================] - 16s 542us/step - loss: 0.0541 - acc: 0.9818\n",
      "Epoch 66/200\n",
      "29325/29325 [==============================] - 16s 541us/step - loss: 0.0533 - acc: 0.9821\n",
      "Epoch 67/200\n",
      "29325/29325 [==============================] - 16s 539us/step - loss: 0.0533 - acc: 0.9820\n",
      "Epoch 68/200\n",
      "29325/29325 [==============================] - 16s 544us/step - loss: 0.0486 - acc: 0.9833\n",
      "Epoch 69/200\n",
      "29325/29325 [==============================] - 16s 550us/step - loss: 0.0583 - acc: 0.9808\n",
      "Epoch 70/200\n",
      "29325/29325 [==============================] - 16s 540us/step - loss: 0.0651 - acc: 0.9781\n",
      "Epoch 71/200\n",
      "29325/29325 [==============================] - 16s 540us/step - loss: 0.0608 - acc: 0.9796\n",
      "Epoch 72/200\n",
      "29325/29325 [==============================] - 16s 540us/step - loss: 0.0494 - acc: 0.9834\n",
      "Epoch 73/200\n",
      "29325/29325 [==============================] - 16s 540us/step - loss: 0.0551 - acc: 0.9820\n",
      "Epoch 74/200\n",
      "29325/29325 [==============================] - 16s 542us/step - loss: 0.0566 - acc: 0.9820\n",
      "Epoch 75/200\n",
      "29325/29325 [==============================] - 17s 595us/step - loss: 0.0611 - acc: 0.9801\n",
      "Epoch 76/200\n",
      "29325/29325 [==============================] - 18s 600us/step - loss: 0.0605 - acc: 0.9797\n",
      "Epoch 77/200\n",
      "29325/29325 [==============================] - 17s 584us/step - loss: 0.0546 - acc: 0.9811\n",
      "Epoch 78/200\n",
      "29325/29325 [==============================] - 17s 584us/step - loss: 0.0557 - acc: 0.9814\n",
      "Epoch 79/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29325/29325 [==============================] - 17s 594us/step - loss: 0.0505 - acc: 0.9829\n",
      "Epoch 80/200\n",
      "29325/29325 [==============================] - 17s 572us/step - loss: 0.0499 - acc: 0.9840\n",
      "Epoch 81/200\n",
      "29325/29325 [==============================] - 16s 554us/step - loss: 0.0531 - acc: 0.9820\n",
      "Epoch 82/200\n",
      "29325/29325 [==============================] - 16s 540us/step - loss: 0.0632 - acc: 0.9791\n",
      "Epoch 83/200\n",
      "29325/29325 [==============================] - 16s 541us/step - loss: 0.0487 - acc: 0.9841\n",
      "Epoch 84/200\n",
      "29325/29325 [==============================] - 16s 540us/step - loss: 0.0446 - acc: 0.9853\n",
      "Epoch 85/200\n",
      "29325/29325 [==============================] - 16s 540us/step - loss: 0.0470 - acc: 0.9850\n",
      "Epoch 86/200\n",
      "29325/29325 [==============================] - 16s 540us/step - loss: 0.0519 - acc: 0.9828\n",
      "Epoch 87/200\n",
      "29325/29325 [==============================] - 16s 540us/step - loss: 0.0664 - acc: 0.9783\n",
      "Epoch 88/200\n",
      "29325/29325 [==============================] - 17s 573us/step - loss: 0.0588 - acc: 0.9803\n",
      "Epoch 89/200\n",
      "29325/29325 [==============================] - 16s 542us/step - loss: 0.0559 - acc: 0.9810\n",
      "Epoch 90/200\n",
      "29325/29325 [==============================] - 16s 542us/step - loss: 0.0430 - acc: 0.9857\n",
      "Epoch 91/200\n",
      "29325/29325 [==============================] - 17s 579us/step - loss: 0.0523 - acc: 0.9834\n",
      "Epoch 92/200\n",
      "29325/29325 [==============================] - 18s 598us/step - loss: 0.0524 - acc: 0.9819\n",
      "Epoch 93/200\n",
      "29325/29325 [==============================] - 17s 571us/step - loss: 0.0567 - acc: 0.9817\n",
      "Epoch 94/200\n",
      "29325/29325 [==============================] - 17s 572us/step - loss: 0.0505 - acc: 0.9837\n",
      "Epoch 95/200\n",
      "29325/29325 [==============================] - 16s 546us/step - loss: 0.0581 - acc: 0.9802\n",
      "Epoch 96/200\n",
      "29325/29325 [==============================] - 16s 549us/step - loss: 0.0421 - acc: 0.9867\n",
      "Epoch 97/200\n",
      "29325/29325 [==============================] - 16s 540us/step - loss: 0.0441 - acc: 0.9853\n",
      "Epoch 98/200\n",
      "29325/29325 [==============================] - 16s 552us/step - loss: 0.0425 - acc: 0.9872\n",
      "Epoch 99/200\n",
      "29325/29325 [==============================] - 16s 561us/step - loss: 0.0419 - acc: 0.9856\n",
      "Epoch 100/200\n",
      "29325/29325 [==============================] - 16s 546us/step - loss: 0.0594 - acc: 0.9813\n",
      "Epoch 101/200\n",
      "29325/29325 [==============================] - 17s 564us/step - loss: 0.0590 - acc: 0.9805\n",
      "Epoch 102/200\n",
      "29325/29325 [==============================] - 17s 564us/step - loss: 0.0510 - acc: 0.9831\n",
      "Epoch 103/200\n",
      "29325/29325 [==============================] - 17s 566us/step - loss: 0.0465 - acc: 0.9849\n",
      "Epoch 104/200\n",
      "29325/29325 [==============================] - 16s 561us/step - loss: 0.0569 - acc: 0.9814\n",
      "Epoch 105/200\n",
      "29325/29325 [==============================] - 16s 549us/step - loss: 0.0441 - acc: 0.9855\n",
      "Epoch 106/200\n",
      "29325/29325 [==============================] - 16s 546us/step - loss: 0.0502 - acc: 0.9837\n",
      "Epoch 107/200\n",
      "29325/29325 [==============================] - 16s 557us/step - loss: 0.0424 - acc: 0.9851\n",
      "Epoch 108/200\n",
      "29325/29325 [==============================] - 16s 559us/step - loss: 0.0432 - acc: 0.9866\n",
      "Epoch 109/200\n",
      "29325/29325 [==============================] - 16s 561us/step - loss: 0.0558 - acc: 0.9822\n",
      "Epoch 110/200\n",
      "29325/29325 [==============================] - 16s 560us/step - loss: 0.0519 - acc: 0.9833\n",
      "Epoch 111/200\n",
      "29325/29325 [==============================] - 16s 558us/step - loss: 0.0478 - acc: 0.9848\n",
      "Epoch 112/200\n",
      "29325/29325 [==============================] - 16s 559us/step - loss: 0.0457 - acc: 0.9846\n",
      "Epoch 113/200\n",
      "29325/29325 [==============================] - 16s 546us/step - loss: 0.0471 - acc: 0.9832\n",
      "Epoch 114/200\n",
      "29325/29325 [==============================] - 16s 544us/step - loss: 0.0497 - acc: 0.9845\n",
      "Epoch 115/200\n",
      "29325/29325 [==============================] - 16s 547us/step - loss: 0.0541 - acc: 0.9838\n",
      "Epoch 116/200\n",
      "29325/29325 [==============================] - 16s 540us/step - loss: 0.0495 - acc: 0.9832\n",
      "Epoch 117/200\n",
      "29325/29325 [==============================] - 16s 540us/step - loss: 0.0446 - acc: 0.9846\n",
      "Epoch 118/200\n",
      "29325/29325 [==============================] - 16s 545us/step - loss: 0.0466 - acc: 0.9846\n",
      "Epoch 119/200\n",
      "29325/29325 [==============================] - 16s 539us/step - loss: 0.0512 - acc: 0.9833\n",
      "Epoch 120/200\n",
      "29325/29325 [==============================] - 16s 540us/step - loss: 0.0521 - acc: 0.9824\n",
      "Epoch 121/200\n",
      "29325/29325 [==============================] - 16s 540us/step - loss: 0.0482 - acc: 0.9847\n",
      "Epoch 122/200\n",
      "29325/29325 [==============================] - 16s 548us/step - loss: 0.0426 - acc: 0.9855\n",
      "Epoch 123/200\n",
      "29325/29325 [==============================] - 16s 554us/step - loss: 0.0441 - acc: 0.9855\n",
      "Epoch 124/200\n",
      "29325/29325 [==============================] - 17s 589us/step - loss: 0.0513 - acc: 0.9828\n",
      "Epoch 125/200\n",
      "29325/29325 [==============================] - 16s 546us/step - loss: 0.0473 - acc: 0.9848\n",
      "Epoch 126/200\n",
      "29325/29325 [==============================] - 16s 540us/step - loss: 0.0513 - acc: 0.9826\n",
      "Epoch 127/200\n",
      "29325/29325 [==============================] - 16s 540us/step - loss: 0.0472 - acc: 0.9847\n",
      "Epoch 128/200\n",
      "29325/29325 [==============================] - 16s 540us/step - loss: 0.0396 - acc: 0.9862\n",
      "Epoch 129/200\n",
      "29325/29325 [==============================] - 16s 540us/step - loss: 0.0440 - acc: 0.9859\n",
      "Epoch 130/200\n",
      "29325/29325 [==============================] - 16s 554us/step - loss: 0.0457 - acc: 0.9853\n",
      "Epoch 131/200\n",
      "29325/29325 [==============================] - 16s 548us/step - loss: 0.0476 - acc: 0.9848\n",
      "Epoch 132/200\n",
      "29325/29325 [==============================] - 17s 576us/step - loss: 0.0493 - acc: 0.9848\n",
      "Epoch 133/200\n",
      "29325/29325 [==============================] - 16s 555us/step - loss: 0.0620 - acc: 0.9797\n",
      "Epoch 134/200\n",
      "29325/29325 [==============================] - 17s 566us/step - loss: 0.0456 - acc: 0.9846\n",
      "Epoch 135/200\n",
      "29325/29325 [==============================] - 17s 581us/step - loss: 0.0387 - acc: 0.9871\n",
      "Epoch 136/200\n",
      "29325/29325 [==============================] - 18s 601us/step - loss: 0.0479 - acc: 0.9847\n",
      "Epoch 137/200\n",
      "29325/29325 [==============================] - 17s 577us/step - loss: 0.0490 - acc: 0.9845\n",
      "Epoch 138/200\n",
      "29325/29325 [==============================] - 17s 585us/step - loss: 0.0399 - acc: 0.9866\n",
      "Epoch 139/200\n",
      "29325/29325 [==============================] - 17s 572us/step - loss: 0.0410 - acc: 0.9864\n",
      "Epoch 140/200\n",
      "29325/29325 [==============================] - 17s 580us/step - loss: 0.0452 - acc: 0.9849\n",
      "Epoch 141/200\n",
      "29325/29325 [==============================] - 17s 576us/step - loss: 0.0542 - acc: 0.9825\n",
      "Epoch 142/200\n",
      "29325/29325 [==============================] - 17s 578us/step - loss: 0.0414 - acc: 0.9865\n",
      "Epoch 143/200\n",
      "29325/29325 [==============================] - 17s 573us/step - loss: 0.0373 - acc: 0.9877\n",
      "Epoch 144/200\n",
      "29325/29325 [==============================] - 17s 579us/step - loss: 0.0431 - acc: 0.9863\n",
      "Epoch 145/200\n",
      "29325/29325 [==============================] - 17s 576us/step - loss: 0.0416 - acc: 0.9858\n",
      "Epoch 146/200\n",
      "29325/29325 [==============================] - 17s 579us/step - loss: 0.0516 - acc: 0.9837\n",
      "Epoch 147/200\n",
      "29325/29325 [==============================] - 17s 583us/step - loss: 0.0461 - acc: 0.9853\n",
      "Epoch 148/200\n",
      "29325/29325 [==============================] - 17s 565us/step - loss: 0.0394 - acc: 0.9875\n",
      "Epoch 149/200\n",
      "29325/29325 [==============================] - 16s 553us/step - loss: 0.0451 - acc: 0.9851\n",
      "Epoch 150/200\n",
      "29325/29325 [==============================] - 16s 557us/step - loss: 0.0474 - acc: 0.9848\n",
      "Epoch 151/200\n",
      "29325/29325 [==============================] - 16s 554us/step - loss: 0.0424 - acc: 0.9852\n",
      "Epoch 152/200\n",
      "29325/29325 [==============================] - 17s 575us/step - loss: 0.0410 - acc: 0.9860\n",
      "Epoch 153/200\n",
      "29325/29325 [==============================] - 17s 580us/step - loss: 0.0430 - acc: 0.9862\n",
      "Epoch 154/200\n",
      "29325/29325 [==============================] - 16s 556us/step - loss: 0.0402 - acc: 0.9883\n",
      "Epoch 155/200\n",
      "29325/29325 [==============================] - 16s 556us/step - loss: 0.0467 - acc: 0.9849\n",
      "Epoch 156/200\n",
      "29325/29325 [==============================] - 17s 595us/step - loss: 0.0409 - acc: 0.9862\n",
      "Epoch 157/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29325/29325 [==============================] - 16s 559us/step - loss: 0.0423 - acc: 0.9861\n",
      "Epoch 158/200\n",
      "29325/29325 [==============================] - 16s 560us/step - loss: 0.0391 - acc: 0.9871\n",
      "Epoch 159/200\n",
      "29325/29325 [==============================] - 16s 558us/step - loss: 0.0459 - acc: 0.9848\n",
      "Epoch 160/200\n",
      "29325/29325 [==============================] - 16s 562us/step - loss: 0.0477 - acc: 0.9838\n",
      "Epoch 161/200\n",
      "29325/29325 [==============================] - 16s 562us/step - loss: 0.0445 - acc: 0.9854\n",
      "Epoch 162/200\n",
      "29325/29325 [==============================] - 16s 550us/step - loss: 0.0442 - acc: 0.9854\n",
      "Epoch 163/200\n",
      "29325/29325 [==============================] - 17s 572us/step - loss: 0.0450 - acc: 0.9848\n",
      "Epoch 164/200\n",
      "29325/29325 [==============================] - 17s 574us/step - loss: 0.0411 - acc: 0.9861\n",
      "Epoch 165/200\n",
      "29325/29325 [==============================] - 16s 557us/step - loss: 0.0429 - acc: 0.9859\n",
      "Epoch 166/200\n",
      "29325/29325 [==============================] - 16s 559us/step - loss: 0.0401 - acc: 0.9869\n",
      "Epoch 167/200\n",
      "29325/29325 [==============================] - 16s 554us/step - loss: 0.0365 - acc: 0.9882\n",
      "Epoch 168/200\n",
      "29325/29325 [==============================] - 16s 557us/step - loss: 0.0450 - acc: 0.9852\n",
      "Epoch 169/200\n",
      "29325/29325 [==============================] - 17s 564us/step - loss: 0.0415 - acc: 0.9867\n",
      "Epoch 170/200\n",
      "29325/29325 [==============================] - 17s 574us/step - loss: 0.0356 - acc: 0.9880\n",
      "Epoch 171/200\n",
      "29325/29325 [==============================] - 17s 568us/step - loss: 0.0501 - acc: 0.9836\n",
      "Epoch 172/200\n",
      "29325/29325 [==============================] - 16s 557us/step - loss: 0.0462 - acc: 0.9851\n",
      "Epoch 173/200\n",
      "29325/29325 [==============================] - 16s 555us/step - loss: 0.0404 - acc: 0.9863\n",
      "Epoch 174/200\n",
      "29325/29325 [==============================] - 16s 543us/step - loss: 0.0368 - acc: 0.9871\n",
      "Epoch 175/200\n",
      "29325/29325 [==============================] - 16s 560us/step - loss: 0.0385 - acc: 0.9878\n",
      "Epoch 176/200\n",
      "29325/29325 [==============================] - 18s 606us/step - loss: 0.0398 - acc: 0.9866\n",
      "Epoch 177/200\n",
      "29325/29325 [==============================] - 18s 630us/step - loss: 0.0410 - acc: 0.9866\n",
      "Epoch 178/200\n",
      "29325/29325 [==============================] - 18s 600us/step - loss: 0.0496 - acc: 0.9840\n",
      "Epoch 179/200\n",
      "29325/29325 [==============================] - 18s 606us/step - loss: 0.0499 - acc: 0.9841\n",
      "Epoch 180/200\n",
      "29325/29325 [==============================] - 17s 594us/step - loss: 0.0425 - acc: 0.9861\n",
      "Epoch 181/200\n",
      "29325/29325 [==============================] - 17s 572us/step - loss: 0.0426 - acc: 0.9863\n",
      "Epoch 182/200\n",
      "29325/29325 [==============================] - 17s 584us/step - loss: 0.0348 - acc: 0.9883\n",
      "Epoch 183/200\n",
      "29325/29325 [==============================] - 17s 584us/step - loss: 0.0406 - acc: 0.9865\n",
      "Epoch 184/200\n",
      "29325/29325 [==============================] - 18s 610us/step - loss: 0.0400 - acc: 0.9866\n",
      "Epoch 185/200\n",
      "29325/29325 [==============================] - 18s 602us/step - loss: 0.0363 - acc: 0.9879\n",
      "Epoch 186/200\n",
      "29325/29325 [==============================] - 18s 628us/step - loss: 0.0405 - acc: 0.9866\n",
      "Epoch 187/200\n",
      "29325/29325 [==============================] - 18s 618us/step - loss: 0.0464 - acc: 0.9846\n",
      "Epoch 188/200\n",
      "29325/29325 [==============================] - 18s 612us/step - loss: 0.0457 - acc: 0.9855\n",
      "Epoch 189/200\n",
      "29325/29325 [==============================] - 18s 611us/step - loss: 0.0420 - acc: 0.9867\n",
      "Epoch 190/200\n",
      "29325/29325 [==============================] - 18s 625us/step - loss: 0.0372 - acc: 0.9880\n",
      "Epoch 191/200\n",
      "29325/29325 [==============================] - 19s 638us/step - loss: 0.0329 - acc: 0.9884\n",
      "Epoch 192/200\n",
      "29325/29325 [==============================] - 19s 640us/step - loss: 0.0420 - acc: 0.9862\n",
      "Epoch 193/200\n",
      "29325/29325 [==============================] - 18s 617us/step - loss: 0.0384 - acc: 0.9873\n",
      "Epoch 194/200\n",
      "29325/29325 [==============================] - 18s 622us/step - loss: 0.0382 - acc: 0.9876\n",
      "Epoch 195/200\n",
      "29325/29325 [==============================] - 18s 609us/step - loss: 0.0422 - acc: 0.9866\n",
      "Epoch 196/200\n",
      "29325/29325 [==============================] - 18s 613us/step - loss: 0.0401 - acc: 0.9868\n",
      "Epoch 197/200\n",
      "29325/29325 [==============================] - 17s 596us/step - loss: 0.0373 - acc: 0.9881\n",
      "Epoch 198/200\n",
      "29325/29325 [==============================] - 18s 621us/step - loss: 0.0389 - acc: 0.9873\n",
      "Epoch 199/200\n",
      "29325/29325 [==============================] - 18s 612us/step - loss: 0.0391 - acc: 0.9872\n",
      "Epoch 200/200\n",
      "29325/29325 [==============================] - 18s 624us/step - loss: 0.0374 - acc: 0.9883\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f622acdeb38>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x, y, batch_size=64, epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nmeyer/Documents/cs155/project3/tensorflow/lib64/python3.6/site-packages/ipykernel_launcher.py:4: RuntimeWarning: divide by zero encountered in log\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shall i compare thee to a summer's day?\n",
      "the roges the glless the hell of berthess thee,\n",
      "ending that this shad all a ther find becuived:\n",
      "that that whos feeds of thy livity that see,\n",
      "that i dery mous scilt, and trish livis be thin,\n",
      "  but thee her reven betterge worth thou less do prese,\n",
      "yet even to thee deep of thee death the sweet sto\n",
      "and look be and i compest that thy hold such varele life:\n",
      "the bot whose for forthing hath he arter\n",
      "that of this wrich comfulfain of thine erdit:\n",
      "that shall will i to for my fauty sell fall well be matine\n",
      "on tun the self the well wasts if less be.\n",
      "  even thee golden thee death he are my from mind,\n",
      "the bothy greth nother with thine eyes,\n",
      "where at an my beauty shall summer's from mind\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(generate_poem())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nmeyer/Documents/cs155/project3/tensorflow/lib64/python3.6/site-packages/ipykernel_launcher.py:4: RuntimeWarning: divide by zero encountered in log\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shall i compare thee to a summer's day?\n",
      "the roges so faur not to my days is graimed:\n",
      "that thou hers be worth than shall (art made,\n",
      "  in makes see sural love the seen tound not thee (see,\n",
      "the notes thy name of summer's dame,\n",
      "the enjures when their doth lie,\n",
      "o so then me, whose thy cath that i hall stame,\n",
      "that moot his cenfort that my fause of say,\n",
      "the boths, whereal which thine and not\n",
      "but my heaven shall in thee blind,\n",
      "  and thee as your pare i hile and tongue,\n",
      "that art my days colline hing and blow:\n",
      "than thou alt faces thee my life of her skill,\n",
      "antiting at my abuse of my life of mine\n",
      "and to be seour with dy sill but dear,\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#seed = \"shall i compare thee to a summer's day?\\nthou art more lovely\"\n",
    "seed = \"shall i compare thee to a summer's day?\\n\"\n",
    "generated = \"\" + seed\n",
    "\n",
    "# Generates 14 lines of poem (More efficient)\n",
    "\n",
    "x_pred = np.zeros((1, char_length - 1, len(chars)))\n",
    "for t, char in enumerate(seed):\n",
    "    x_pred[0, t, char_indices[char]] = 1.\n",
    "preds = model.predict(x_pred, verbose=0)[0]\n",
    "\n",
    "for i in range(14):\n",
    "    while True:\n",
    "        next_index = sample(preds, temperature=0.5)\n",
    "        next_char = indices_char[next_index]\n",
    "        \n",
    "        x_pred[0, :-1, :] = x_pred[:, 1:, :]\n",
    "        x_pred[0, -1, :] = np.zeros(len(chars))\n",
    "        x_pred[0, -1, char_indices[next_char]] = 1.\n",
    "        preds = model.predict(x_pred, verbose=0)[0]\n",
    "        \n",
    "        generated += next_char\n",
    "        if next_char == \"\\n\":\n",
    "            #print(generated)\n",
    "            #generated = \"\"\n",
    "            break\n",
    "        #generated += next_char\n",
    "        \n",
    "\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying training with a lower dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = Sequential()\n",
    "\n",
    "model2.add(LSTM(200, input_shape=(char_length - 1, len(chars))))\n",
    "model2.add(Dense(200))\n",
    "model2.add(Activation('relu'))\n",
    "model2.add(Dropout(0.1))\n",
    "\n",
    "model2.add(Dense(len(chars)))\n",
    "model2.add(Activation('softmax'))\n",
    "model2.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "29325/29325 [==============================] - 18s 598us/step - loss: 2.7180 - acc: 0.2372\n",
      "Epoch 2/200\n",
      "29325/29325 [==============================] - 17s 570us/step - loss: 2.2105 - acc: 0.3525\n",
      "Epoch 3/200\n",
      "29325/29325 [==============================] - 17s 576us/step - loss: 2.0421 - acc: 0.3948\n",
      "Epoch 4/200\n",
      "29325/29325 [==============================] - 17s 564us/step - loss: 1.9343 - acc: 0.4237\n",
      "Epoch 5/200\n",
      "29325/29325 [==============================] - 16s 548us/step - loss: 1.8471 - acc: 0.4431\n",
      "Epoch 6/200\n",
      "29325/29325 [==============================] - 16s 546us/step - loss: 1.7729 - acc: 0.4648\n",
      "Epoch 7/200\n",
      "29325/29325 [==============================] - 16s 546us/step - loss: 1.7089 - acc: 0.4789\n",
      "Epoch 8/200\n",
      "29325/29325 [==============================] - 16s 547us/step - loss: 1.6488 - acc: 0.4941\n",
      "Epoch 9/200\n",
      "29325/29325 [==============================] - 16s 547us/step - loss: 1.5852 - acc: 0.5079\n",
      "Epoch 10/200\n",
      "29325/29325 [==============================] - 16s 547us/step - loss: 1.5201 - acc: 0.5243\n",
      "Epoch 11/200\n",
      "29325/29325 [==============================] - 16s 550us/step - loss: 1.4569 - acc: 0.5421\n",
      "Epoch 12/200\n",
      "29325/29325 [==============================] - 16s 546us/step - loss: 1.3880 - acc: 0.5597\n",
      "Epoch 13/200\n",
      "29325/29325 [==============================] - 16s 559us/step - loss: 1.3082 - acc: 0.5837\n",
      "Epoch 14/200\n",
      "29325/29325 [==============================] - 16s 554us/step - loss: 1.2265 - acc: 0.6062\n",
      "Epoch 15/200\n",
      "29325/29325 [==============================] - 17s 565us/step - loss: 1.1489 - acc: 0.6308\n",
      "Epoch 16/200\n",
      "29325/29325 [==============================] - 16s 555us/step - loss: 1.0629 - acc: 0.6558\n",
      "Epoch 17/200\n",
      "29325/29325 [==============================] - 17s 569us/step - loss: 0.9783 - acc: 0.6836\n",
      "Epoch 18/200\n",
      "29325/29325 [==============================] - 16s 551us/step - loss: 0.8944 - acc: 0.7092\n",
      "Epoch 19/200\n",
      "29325/29325 [==============================] - 16s 556us/step - loss: 0.8169 - acc: 0.7343\n",
      "Epoch 20/200\n",
      "29325/29325 [==============================] - 17s 591us/step - loss: 0.7378 - acc: 0.7609\n",
      "Epoch 21/200\n",
      "29325/29325 [==============================] - 17s 593us/step - loss: 0.6626 - acc: 0.7856\n",
      "Epoch 22/200\n",
      "29325/29325 [==============================] - 17s 565us/step - loss: 0.5997 - acc: 0.8057\n",
      "Epoch 23/200\n",
      "29325/29325 [==============================] - 16s 552us/step - loss: 0.5392 - acc: 0.8249\n",
      "Epoch 24/200\n",
      "29325/29325 [==============================] - 16s 551us/step - loss: 0.4859 - acc: 0.8438\n",
      "Epoch 25/200\n",
      "29325/29325 [==============================] - 16s 547us/step - loss: 0.4294 - acc: 0.8630\n",
      "Epoch 26/200\n",
      "29325/29325 [==============================] - 16s 550us/step - loss: 0.3929 - acc: 0.8717\n",
      "Epoch 27/200\n",
      "29325/29325 [==============================] - 16s 546us/step - loss: 0.3483 - acc: 0.8917\n",
      "Epoch 28/200\n",
      "29325/29325 [==============================] - 16s 557us/step - loss: 0.3227 - acc: 0.8965\n",
      "Epoch 29/200\n",
      "29325/29325 [==============================] - 17s 571us/step - loss: 0.2877 - acc: 0.9085\n",
      "Epoch 30/200\n",
      "29325/29325 [==============================] - 16s 546us/step - loss: 0.2757 - acc: 0.9109\n",
      "Epoch 31/200\n",
      "29325/29325 [==============================] - 16s 547us/step - loss: 0.2393 - acc: 0.9253\n",
      "Epoch 32/200\n",
      "29325/29325 [==============================] - 16s 547us/step - loss: 0.2413 - acc: 0.9221\n",
      "Epoch 33/200\n",
      "29325/29325 [==============================] - 16s 553us/step - loss: 0.2096 - acc: 0.9324\n",
      "Epoch 34/200\n",
      "29325/29325 [==============================] - 16s 562us/step - loss: 0.2014 - acc: 0.9371\n",
      "Epoch 35/200\n",
      "29325/29325 [==============================] - 16s 552us/step - loss: 0.1809 - acc: 0.9420\n",
      "Epoch 36/200\n",
      "29325/29325 [==============================] - 16s 550us/step - loss: 0.1772 - acc: 0.9441\n",
      "Epoch 37/200\n",
      "29325/29325 [==============================] - 16s 552us/step - loss: 0.1748 - acc: 0.9437\n",
      "Epoch 38/200\n",
      "29325/29325 [==============================] - 16s 545us/step - loss: 0.1729 - acc: 0.9439\n",
      "Epoch 39/200\n",
      "29325/29325 [==============================] - 16s 546us/step - loss: 0.1560 - acc: 0.9507\n",
      "Epoch 40/200\n",
      "29325/29325 [==============================] - 16s 551us/step - loss: 0.1477 - acc: 0.9536\n",
      "Epoch 41/200\n",
      "29325/29325 [==============================] - 17s 593us/step - loss: 0.1552 - acc: 0.9497\n",
      "Epoch 42/200\n",
      "29325/29325 [==============================] - 18s 603us/step - loss: 0.1575 - acc: 0.9490\n",
      "Epoch 43/200\n",
      "29325/29325 [==============================] - 18s 605us/step - loss: 0.1378 - acc: 0.9566\n",
      "Epoch 44/200\n",
      "29325/29325 [==============================] - 18s 607us/step - loss: 0.1249 - acc: 0.9607\n",
      "Epoch 45/200\n",
      "29325/29325 [==============================] - 16s 559us/step - loss: 0.1295 - acc: 0.9576\n",
      "Epoch 46/200\n",
      "29325/29325 [==============================] - 16s 546us/step - loss: 0.1423 - acc: 0.9527\n",
      "Epoch 47/200\n",
      "29325/29325 [==============================] - 16s 550us/step - loss: 0.1220 - acc: 0.9613\n",
      "Epoch 48/200\n",
      "29325/29325 [==============================] - 16s 560us/step - loss: 0.1196 - acc: 0.9611\n",
      "Epoch 49/200\n",
      "29325/29325 [==============================] - 18s 598us/step - loss: 0.1254 - acc: 0.9596\n",
      "Epoch 50/200\n",
      "29325/29325 [==============================] - 17s 571us/step - loss: 0.1203 - acc: 0.9605\n",
      "Epoch 51/200\n",
      "29325/29325 [==============================] - 17s 565us/step - loss: 0.1275 - acc: 0.9588\n",
      "Epoch 52/200\n",
      "29325/29325 [==============================] - 17s 574us/step - loss: 0.1032 - acc: 0.9669\n",
      "Epoch 53/200\n",
      "29325/29325 [==============================] - 17s 571us/step - loss: 0.0999 - acc: 0.9685\n",
      "Epoch 54/200\n",
      "29325/29325 [==============================] - 17s 568us/step - loss: 0.1143 - acc: 0.9625\n",
      "Epoch 55/200\n",
      "29325/29325 [==============================] - 17s 570us/step - loss: 0.1187 - acc: 0.9608\n",
      "Epoch 56/200\n",
      "29325/29325 [==============================] - 17s 568us/step - loss: 0.1089 - acc: 0.9649\n",
      "Epoch 57/200\n",
      "29325/29325 [==============================] - 17s 576us/step - loss: 0.1048 - acc: 0.9652\n",
      "Epoch 58/200\n",
      "29325/29325 [==============================] - 17s 573us/step - loss: 0.1090 - acc: 0.9637\n",
      "Epoch 59/200\n",
      "29325/29325 [==============================] - 17s 571us/step - loss: 0.1111 - acc: 0.9644\n",
      "Epoch 60/200\n",
      "29325/29325 [==============================] - 17s 569us/step - loss: 0.1039 - acc: 0.9665\n",
      "Epoch 61/200\n",
      "29325/29325 [==============================] - 17s 574us/step - loss: 0.0814 - acc: 0.9757\n",
      "Epoch 62/200\n",
      "29325/29325 [==============================] - 17s 572us/step - loss: 0.1009 - acc: 0.9678\n",
      "Epoch 63/200\n",
      "29325/29325 [==============================] - 17s 574us/step - loss: 0.1145 - acc: 0.9617\n",
      "Epoch 64/200\n",
      "29325/29325 [==============================] - 17s 569us/step - loss: 0.0836 - acc: 0.9733\n",
      "Epoch 65/200\n",
      "29325/29325 [==============================] - 17s 569us/step - loss: 0.0822 - acc: 0.9737\n",
      "Epoch 66/200\n",
      "29325/29325 [==============================] - 17s 571us/step - loss: 0.0937 - acc: 0.9703\n",
      "Epoch 67/200\n",
      "29325/29325 [==============================] - 17s 580us/step - loss: 0.1093 - acc: 0.9629\n",
      "Epoch 68/200\n",
      "29325/29325 [==============================] - 17s 576us/step - loss: 0.0996 - acc: 0.9671\n",
      "Epoch 69/200\n",
      "29325/29325 [==============================] - 17s 568us/step - loss: 0.0949 - acc: 0.9683\n",
      "Epoch 70/200\n",
      "29325/29325 [==============================] - 17s 568us/step - loss: 0.0825 - acc: 0.9732\n",
      "Epoch 71/200\n",
      "29325/29325 [==============================] - 17s 568us/step - loss: 0.0828 - acc: 0.9744\n",
      "Epoch 72/200\n",
      "29325/29325 [==============================] - 17s 569us/step - loss: 0.0930 - acc: 0.9688\n",
      "Epoch 73/200\n",
      "29325/29325 [==============================] - 17s 567us/step - loss: 0.0929 - acc: 0.9692\n",
      "Epoch 74/200\n",
      "29325/29325 [==============================] - 17s 569us/step - loss: 0.0934 - acc: 0.9694\n",
      "Epoch 75/200\n",
      "29325/29325 [==============================] - 17s 575us/step - loss: 0.0846 - acc: 0.9732\n",
      "Epoch 76/200\n",
      "29325/29325 [==============================] - 17s 578us/step - loss: 0.0803 - acc: 0.9740\n",
      "Epoch 77/200\n",
      "29325/29325 [==============================] - 16s 547us/step - loss: 0.0719 - acc: 0.9761\n",
      "Epoch 78/200\n",
      "29325/29325 [==============================] - 16s 549us/step - loss: 0.0785 - acc: 0.9751\n",
      "Epoch 79/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29325/29325 [==============================] - 16s 551us/step - loss: 0.0902 - acc: 0.9710\n",
      "Epoch 80/200\n",
      "29325/29325 [==============================] - 16s 548us/step - loss: 0.0853 - acc: 0.9729\n",
      "Epoch 81/200\n",
      "29325/29325 [==============================] - 16s 549us/step - loss: 0.0750 - acc: 0.9753\n",
      "Epoch 82/200\n",
      "29325/29325 [==============================] - 16s 546us/step - loss: 0.0710 - acc: 0.9779\n",
      "Epoch 83/200\n",
      "29325/29325 [==============================] - 16s 553us/step - loss: 0.0755 - acc: 0.9742\n",
      "Epoch 84/200\n",
      "29325/29325 [==============================] - 16s 544us/step - loss: 0.1076 - acc: 0.9645\n",
      "Epoch 85/200\n",
      "29325/29325 [==============================] - 16s 548us/step - loss: 0.0983 - acc: 0.9678\n",
      "Epoch 86/200\n",
      "29325/29325 [==============================] - 16s 545us/step - loss: 0.0637 - acc: 0.9790\n",
      "Epoch 87/200\n",
      "29325/29325 [==============================] - 16s 545us/step - loss: 0.0531 - acc: 0.9831\n",
      "Epoch 88/200\n",
      "29325/29325 [==============================] - 16s 545us/step - loss: 0.0659 - acc: 0.9782\n",
      "Epoch 89/200\n",
      "29325/29325 [==============================] - 16s 546us/step - loss: 0.0909 - acc: 0.9694\n",
      "Epoch 90/200\n",
      "29325/29325 [==============================] - 17s 578us/step - loss: 0.0858 - acc: 0.9715\n",
      "Epoch 91/200\n",
      "29325/29325 [==============================] - 16s 561us/step - loss: 0.0693 - acc: 0.9775\n",
      "Epoch 92/200\n",
      "29325/29325 [==============================] - 16s 549us/step - loss: 0.0767 - acc: 0.9749\n",
      "Epoch 93/200\n",
      "29325/29325 [==============================] - 16s 544us/step - loss: 0.0714 - acc: 0.9759\n",
      "Epoch 94/200\n",
      "29325/29325 [==============================] - 16s 559us/step - loss: 0.0714 - acc: 0.9755\n",
      "Epoch 95/200\n",
      "29325/29325 [==============================] - 16s 550us/step - loss: 0.0875 - acc: 0.9712\n",
      "Epoch 96/200\n",
      "29325/29325 [==============================] - 16s 544us/step - loss: 0.0656 - acc: 0.9795\n",
      "Epoch 97/200\n",
      "29325/29325 [==============================] - 16s 546us/step - loss: 0.0767 - acc: 0.9750\n",
      "Epoch 98/200\n",
      "29325/29325 [==============================] - 16s 548us/step - loss: 0.0640 - acc: 0.9788\n",
      "Epoch 99/200\n",
      "29325/29325 [==============================] - 16s 546us/step - loss: 0.0672 - acc: 0.9785\n",
      "Epoch 100/200\n",
      "29325/29325 [==============================] - 16s 541us/step - loss: 0.0703 - acc: 0.9769\n",
      "Epoch 101/200\n",
      "29325/29325 [==============================] - 16s 541us/step - loss: 0.0729 - acc: 0.9757\n",
      "Epoch 102/200\n",
      "29325/29325 [==============================] - 16s 545us/step - loss: 0.0653 - acc: 0.9780\n",
      "Epoch 103/200\n",
      "29325/29325 [==============================] - 16s 545us/step - loss: 0.0713 - acc: 0.9763\n",
      "Epoch 104/200\n",
      "29325/29325 [==============================] - 16s 544us/step - loss: 0.0672 - acc: 0.9779\n",
      "Epoch 105/200\n",
      "29325/29325 [==============================] - 16s 549us/step - loss: 0.0781 - acc: 0.9742\n",
      "Epoch 106/200\n",
      "29325/29325 [==============================] - 16s 544us/step - loss: 0.0710 - acc: 0.9768\n",
      "Epoch 107/200\n",
      "29325/29325 [==============================] - 16s 545us/step - loss: 0.0556 - acc: 0.9815\n",
      "Epoch 108/200\n",
      "29325/29325 [==============================] - 16s 544us/step - loss: 0.0563 - acc: 0.9819\n",
      "Epoch 109/200\n",
      "29325/29325 [==============================] - 16s 543us/step - loss: 0.0569 - acc: 0.9814\n",
      "Epoch 110/200\n",
      "29325/29325 [==============================] - 18s 608us/step - loss: 0.0671 - acc: 0.9788\n",
      "Epoch 111/200\n",
      "29325/29325 [==============================] - 16s 557us/step - loss: 0.0742 - acc: 0.9757\n",
      "Epoch 112/200\n",
      "29325/29325 [==============================] - 16s 548us/step - loss: 0.0632 - acc: 0.9794\n",
      "Epoch 113/200\n",
      "29325/29325 [==============================] - 16s 548us/step - loss: 0.0610 - acc: 0.9798\n",
      "Epoch 114/200\n",
      "29325/29325 [==============================] - 16s 552us/step - loss: 0.0679 - acc: 0.9781\n",
      "Epoch 115/200\n",
      "29325/29325 [==============================] - 16s 549us/step - loss: 0.0642 - acc: 0.9799\n",
      "Epoch 116/200\n",
      "29325/29325 [==============================] - 16s 559us/step - loss: 0.0655 - acc: 0.9783\n",
      "Epoch 117/200\n",
      "29325/29325 [==============================] - 16s 553us/step - loss: 0.0533 - acc: 0.9823\n",
      "Epoch 118/200\n",
      "29325/29325 [==============================] - 16s 560us/step - loss: 0.0537 - acc: 0.9824\n",
      "Epoch 119/200\n",
      "29325/29325 [==============================] - 16s 550us/step - loss: 0.0519 - acc: 0.9837\n",
      "Epoch 120/200\n",
      "29325/29325 [==============================] - 16s 551us/step - loss: 0.0661 - acc: 0.9787\n",
      "Epoch 121/200\n",
      "29325/29325 [==============================] - 17s 572us/step - loss: 0.0694 - acc: 0.9779\n",
      "Epoch 122/200\n",
      "29325/29325 [==============================] - 17s 565us/step - loss: 0.0675 - acc: 0.9776\n",
      "Epoch 123/200\n",
      "29325/29325 [==============================] - 17s 573us/step - loss: 0.0546 - acc: 0.9825\n",
      "Epoch 124/200\n",
      "29325/29325 [==============================] - 16s 546us/step - loss: 0.0577 - acc: 0.9807\n",
      "Epoch 125/200\n",
      "29325/29325 [==============================] - 16s 555us/step - loss: 0.0697 - acc: 0.9766\n",
      "Epoch 126/200\n",
      "29325/29325 [==============================] - 16s 558us/step - loss: 0.0622 - acc: 0.9792\n",
      "Epoch 127/200\n",
      "29325/29325 [==============================] - 16s 545us/step - loss: 0.0573 - acc: 0.9816\n",
      "Epoch 128/200\n",
      "29325/29325 [==============================] - 17s 574us/step - loss: 0.0568 - acc: 0.9814\n",
      "Epoch 129/200\n",
      "29325/29325 [==============================] - 16s 541us/step - loss: 0.0573 - acc: 0.9803\n",
      "Epoch 130/200\n",
      "29325/29325 [==============================] - 16s 542us/step - loss: 0.0528 - acc: 0.9832\n",
      "Epoch 131/200\n",
      "29325/29325 [==============================] - 17s 590us/step - loss: 0.0647 - acc: 0.9781\n",
      "Epoch 132/200\n",
      "29325/29325 [==============================] - 16s 555us/step - loss: 0.0603 - acc: 0.9802\n",
      "Epoch 133/200\n",
      "29325/29325 [==============================] - 17s 588us/step - loss: 0.0563 - acc: 0.9814\n",
      "Epoch 134/200\n",
      "29325/29325 [==============================] - 16s 549us/step - loss: 0.0488 - acc: 0.9847\n",
      "Epoch 135/200\n",
      "29325/29325 [==============================] - 16s 545us/step - loss: 0.0491 - acc: 0.9844\n",
      "Epoch 136/200\n",
      "29325/29325 [==============================] - 16s 550us/step - loss: 0.0599 - acc: 0.9804\n",
      "Epoch 137/200\n",
      "29325/29325 [==============================] - 16s 543us/step - loss: 0.0583 - acc: 0.9803\n",
      "Epoch 138/200\n",
      "29325/29325 [==============================] - 16s 542us/step - loss: 0.0570 - acc: 0.9816\n",
      "Epoch 139/200\n",
      "29325/29325 [==============================] - 16s 545us/step - loss: 0.0577 - acc: 0.9804\n",
      "Epoch 140/200\n",
      "29325/29325 [==============================] - 16s 543us/step - loss: 0.0522 - acc: 0.9832\n",
      "Epoch 141/200\n",
      "29325/29325 [==============================] - 16s 544us/step - loss: 0.0669 - acc: 0.9778\n",
      "Epoch 142/200\n",
      "29325/29325 [==============================] - 16s 547us/step - loss: 0.0601 - acc: 0.9794\n",
      "Epoch 143/200\n",
      "29325/29325 [==============================] - 16s 549us/step - loss: 0.0436 - acc: 0.9859\n",
      "Epoch 144/200\n",
      "29325/29325 [==============================] - 16s 555us/step - loss: 0.0451 - acc: 0.9855\n",
      "Epoch 145/200\n",
      "29325/29325 [==============================] - 16s 555us/step - loss: 0.0535 - acc: 0.9819\n",
      "Epoch 146/200\n",
      "29325/29325 [==============================] - 16s 543us/step - loss: 0.0505 - acc: 0.9833\n",
      "Epoch 147/200\n",
      "29325/29325 [==============================] - 16s 544us/step - loss: 0.0575 - acc: 0.9807\n",
      "Epoch 148/200\n",
      "29325/29325 [==============================] - 16s 545us/step - loss: 0.0563 - acc: 0.9817\n",
      "Epoch 149/200\n",
      "29325/29325 [==============================] - 16s 550us/step - loss: 0.0568 - acc: 0.9809\n",
      "Epoch 150/200\n",
      "29325/29325 [==============================] - 16s 551us/step - loss: 0.0525 - acc: 0.9823\n",
      "Epoch 151/200\n",
      "29325/29325 [==============================] - 16s 545us/step - loss: 0.0442 - acc: 0.9859\n",
      "Epoch 152/200\n",
      "29325/29325 [==============================] - 16s 542us/step - loss: 0.0481 - acc: 0.9842\n",
      "Epoch 153/200\n",
      "29325/29325 [==============================] - 16s 542us/step - loss: 0.0517 - acc: 0.9827\n",
      "Epoch 154/200\n",
      "29325/29325 [==============================] - 16s 543us/step - loss: 0.0548 - acc: 0.9821\n",
      "Epoch 155/200\n",
      "29325/29325 [==============================] - 16s 553us/step - loss: 0.0511 - acc: 0.9831\n",
      "Epoch 156/200\n",
      "29325/29325 [==============================] - 16s 542us/step - loss: 0.0519 - acc: 0.9829\n",
      "Epoch 157/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29325/29325 [==============================] - 16s 545us/step - loss: 0.0625 - acc: 0.9795\n",
      "Epoch 158/200\n",
      "29325/29325 [==============================] - 16s 542us/step - loss: 0.0464 - acc: 0.9844\n",
      "Epoch 159/200\n",
      "29325/29325 [==============================] - 16s 542us/step - loss: 0.0451 - acc: 0.9849\n",
      "Epoch 160/200\n",
      "29325/29325 [==============================] - 16s 541us/step - loss: 0.0518 - acc: 0.9832\n",
      "Epoch 161/200\n",
      "29325/29325 [==============================] - 16s 541us/step - loss: 0.0511 - acc: 0.9836\n",
      "Epoch 162/200\n",
      "29325/29325 [==============================] - 16s 541us/step - loss: 0.0597 - acc: 0.9808\n",
      "Epoch 163/200\n",
      "29325/29325 [==============================] - 16s 541us/step - loss: 0.0454 - acc: 0.9839\n",
      "Epoch 164/200\n",
      "29325/29325 [==============================] - 16s 542us/step - loss: 0.0452 - acc: 0.9847\n",
      "Epoch 165/200\n",
      "29325/29325 [==============================] - 16s 548us/step - loss: 0.0490 - acc: 0.9844\n",
      "Epoch 166/200\n",
      "29325/29325 [==============================] - 16s 544us/step - loss: 0.0505 - acc: 0.9836\n",
      "Epoch 167/200\n",
      "29325/29325 [==============================] - 16s 542us/step - loss: 0.0552 - acc: 0.9822\n",
      "Epoch 168/200\n",
      "29325/29325 [==============================] - 16s 542us/step - loss: 0.0389 - acc: 0.9876\n",
      "Epoch 169/200\n",
      "29325/29325 [==============================] - 16s 542us/step - loss: 0.0421 - acc: 0.9866\n",
      "Epoch 170/200\n",
      "29325/29325 [==============================] - 16s 541us/step - loss: 0.0388 - acc: 0.9872\n",
      "Epoch 171/200\n",
      "29325/29325 [==============================] - 16s 544us/step - loss: 0.0575 - acc: 0.9813\n",
      "Epoch 172/200\n",
      "29325/29325 [==============================] - 16s 541us/step - loss: 0.0560 - acc: 0.9813\n",
      "Epoch 173/200\n",
      "29325/29325 [==============================] - 16s 541us/step - loss: 0.0479 - acc: 0.9841\n",
      "Epoch 174/200\n",
      "29325/29325 [==============================] - 16s 546us/step - loss: 0.0404 - acc: 0.9860\n",
      "Epoch 175/200\n",
      "29325/29325 [==============================] - 16s 546us/step - loss: 0.0510 - acc: 0.9831\n",
      "Epoch 176/200\n",
      "29325/29325 [==============================] - 16s 546us/step - loss: 0.0430 - acc: 0.9870\n",
      "Epoch 177/200\n",
      "29325/29325 [==============================] - 16s 542us/step - loss: 0.0369 - acc: 0.9883\n",
      "Epoch 178/200\n",
      "29325/29325 [==============================] - 16s 541us/step - loss: 0.0429 - acc: 0.9861\n",
      "Epoch 179/200\n",
      "29325/29325 [==============================] - 16s 541us/step - loss: 0.0435 - acc: 0.9849\n",
      "Epoch 180/200\n",
      "29325/29325 [==============================] - 16s 549us/step - loss: 0.0442 - acc: 0.9860\n",
      "Epoch 181/200\n",
      "29325/29325 [==============================] - 17s 581us/step - loss: 0.0518 - acc: 0.9834\n",
      "Epoch 182/200\n",
      "29325/29325 [==============================] - 16s 553us/step - loss: 0.0517 - acc: 0.9827\n",
      "Epoch 183/200\n",
      "29325/29325 [==============================] - 17s 594us/step - loss: 0.0459 - acc: 0.9852\n",
      "Epoch 184/200\n",
      "29325/29325 [==============================] - 17s 571us/step - loss: 0.0457 - acc: 0.9847\n",
      "Epoch 185/200\n",
      "29325/29325 [==============================] - 16s 552us/step - loss: 0.0403 - acc: 0.9869\n",
      "Epoch 186/200\n",
      "29325/29325 [==============================] - 16s 547us/step - loss: 0.0432 - acc: 0.9861\n",
      "Epoch 187/200\n",
      "29325/29325 [==============================] - 16s 547us/step - loss: 0.0532 - acc: 0.9823\n",
      "Epoch 188/200\n",
      "29325/29325 [==============================] - 16s 544us/step - loss: 0.0416 - acc: 0.9859\n",
      "Epoch 189/200\n",
      "29325/29325 [==============================] - 16s 543us/step - loss: 0.0481 - acc: 0.9848\n",
      "Epoch 190/200\n",
      "29325/29325 [==============================] - 16s 544us/step - loss: 0.0461 - acc: 0.9851\n",
      "Epoch 191/200\n",
      "29325/29325 [==============================] - 16s 544us/step - loss: 0.0419 - acc: 0.9866\n",
      "Epoch 192/200\n",
      "29325/29325 [==============================] - 16s 547us/step - loss: 0.0371 - acc: 0.9878\n",
      "Epoch 193/200\n",
      "29325/29325 [==============================] - 16s 550us/step - loss: 0.0370 - acc: 0.9877\n",
      "Epoch 194/200\n",
      "29325/29325 [==============================] - 16s 557us/step - loss: 0.0455 - acc: 0.9847\n",
      "Epoch 195/200\n",
      "29325/29325 [==============================] - 16s 563us/step - loss: 0.0540 - acc: 0.9824\n",
      "Epoch 196/200\n",
      "29325/29325 [==============================] - 17s 564us/step - loss: 0.0467 - acc: 0.9845\n",
      "Epoch 197/200\n",
      "29325/29325 [==============================] - 16s 542us/step - loss: 0.0441 - acc: 0.9862\n",
      "Epoch 198/200\n",
      "29325/29325 [==============================] - 16s 543us/step - loss: 0.0391 - acc: 0.9868\n",
      "Epoch 199/200\n",
      "29325/29325 [==============================] - 16s 559us/step - loss: 0.0442 - acc: 0.9862\n",
      "Epoch 200/200\n",
      "29325/29325 [==============================] - 16s 551us/step - loss: 0.0400 - acc: 0.9866\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f61d1fb2898>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.fit(x, y, batch_size=64, epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nmeyer/Documents/cs155/project3/tensorflow/lib64/python3.6/site-packages/ipykernel_launcher.py:4: RuntimeWarning: divide by zero encountered in log\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shall i compare thee to a summer's day?\n",
      "the worly bast love be me do i  hos vord,\n",
      "and unstand not strande of my my heart,\n",
      "making the sward's blappecassed bathous do thy no brow\n",
      "his did a beautes (ull bate, with i him me spine,\n",
      "when all my art's ppicte with claker thee,\n",
      "  then of you should prurant's sinkle pride,\n",
      "the porroumen him and world (soor frow,\n",
      "have i some then mer'lecoone why heart with thy ste,\n",
      "and seed'st thy self comfed thy self rerume.\n",
      "o that of ment'r selfly hide sick look.\n",
      "o make me glous in the beauty on my love's haped,\n",
      "what i hast his flould in his plessling,\n",
      "and for my sight obrains of thou be during,\n",
      "thy self reyiquent change, or truth he dlave,\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#seed = \"shall i compare thee to a summer's day?\\nthou art more lovely\"\n",
    "seed = \"shall i compare thee to a summer's day?\\n\"\n",
    "generated = \"\" + seed\n",
    "\n",
    "# Generates 14 lines of poem (More efficient)\n",
    "\n",
    "x_pred = np.zeros((1, char_length - 1, len(chars)))\n",
    "for t, char in enumerate(seed):\n",
    "    x_pred[0, t, char_indices[char]] = 1.\n",
    "preds = model2.predict(x_pred, verbose=0)[0]\n",
    "\n",
    "for i in range(14):\n",
    "    while True:\n",
    "        next_index = sample(preds, temperature=0.5)\n",
    "        next_char = indices_char[next_index]\n",
    "        \n",
    "        x_pred[0, :-1, :] = x_pred[:, 1:, :]\n",
    "        x_pred[0, -1, :] = np.zeros(len(chars))\n",
    "        x_pred[0, -1, char_indices[next_char]] = 1.\n",
    "        preds = model2.predict(x_pred, verbose=0)[0]\n",
    "        \n",
    "        generated += next_char\n",
    "        if next_char == \"\\n\":\n",
    "            #print(generated)\n",
    "            #generated = \"\"\n",
    "            break\n",
    "        #generated += next_char\n",
    "        \n",
    "\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now trying a higher dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = Sequential()\n",
    "\n",
    "model3.add(LSTM(200, input_shape=(char_length - 1, len(chars))))\n",
    "model3.add(Dense(200))\n",
    "model3.add(Activation('relu'))\n",
    "model3.add(Dropout(0.5))\n",
    "\n",
    "model3.add(Dense(len(chars)))\n",
    "model3.add(Activation('softmax'))\n",
    "model3.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "29325/29325 [==============================] - 18s 618us/step - loss: 2.8100 - acc: 0.2192\n",
      "Epoch 2/200\n",
      "29325/29325 [==============================] - 16s 550us/step - loss: 2.3040 - acc: 0.3287\n",
      "Epoch 3/200\n",
      "29325/29325 [==============================] - 17s 572us/step - loss: 2.1401 - acc: 0.3754\n",
      "Epoch 4/200\n",
      "29325/29325 [==============================] - 16s 562us/step - loss: 2.0329 - acc: 0.4011\n",
      "Epoch 5/200\n",
      "29325/29325 [==============================] - 18s 604us/step - loss: 1.9620 - acc: 0.4182\n",
      "Epoch 6/200\n",
      "29325/29325 [==============================] - 16s 554us/step - loss: 1.8989 - acc: 0.4322\n",
      "Epoch 7/200\n",
      "29325/29325 [==============================] - 17s 587us/step - loss: 1.8357 - acc: 0.4507\n",
      "Epoch 8/200\n",
      "29325/29325 [==============================] - 17s 566us/step - loss: 1.7829 - acc: 0.4646\n",
      "Epoch 9/200\n",
      "29325/29325 [==============================] - 17s 584us/step - loss: 1.7339 - acc: 0.4726\n",
      "Epoch 10/200\n",
      "29325/29325 [==============================] - 18s 605us/step - loss: 1.6920 - acc: 0.4860\n",
      "Epoch 11/200\n",
      "29325/29325 [==============================] - 17s 595us/step - loss: 1.6472 - acc: 0.4960\n",
      "Epoch 12/200\n",
      "29325/29325 [==============================] - 18s 623us/step - loss: 1.5973 - acc: 0.5072\n",
      "Epoch 13/200\n",
      "29325/29325 [==============================] - 16s 545us/step - loss: 1.5596 - acc: 0.5148\n",
      "Epoch 14/200\n",
      "29325/29325 [==============================] - 17s 574us/step - loss: 1.5043 - acc: 0.5309\n",
      "Epoch 15/200\n",
      "29325/29325 [==============================] - 17s 587us/step - loss: 1.4602 - acc: 0.5397\n",
      "Epoch 16/200\n",
      "29325/29325 [==============================] - 17s 574us/step - loss: 1.4050 - acc: 0.5547\n",
      "Epoch 17/200\n",
      "29325/29325 [==============================] - 17s 574us/step - loss: 1.3587 - acc: 0.5643\n",
      "Epoch 18/200\n",
      "29325/29325 [==============================] - 17s 580us/step - loss: 1.3076 - acc: 0.5809\n",
      "Epoch 19/200\n",
      "29325/29325 [==============================] - 18s 608us/step - loss: 1.2602 - acc: 0.5936\n",
      "Epoch 20/200\n",
      "29325/29325 [==============================] - 17s 569us/step - loss: 1.2061 - acc: 0.6072\n",
      "Epoch 21/200\n",
      "29325/29325 [==============================] - 16s 544us/step - loss: 1.1598 - acc: 0.6210\n",
      "Epoch 22/200\n",
      "29325/29325 [==============================] - 16s 543us/step - loss: 1.1096 - acc: 0.6342\n",
      "Epoch 23/200\n",
      "29325/29325 [==============================] - 16s 542us/step - loss: 1.0718 - acc: 0.6477\n",
      "Epoch 24/200\n",
      "29325/29325 [==============================] - 16s 543us/step - loss: 1.0210 - acc: 0.6611\n",
      "Epoch 25/200\n",
      "29325/29325 [==============================] - 16s 541us/step - loss: 0.9798 - acc: 0.6744\n",
      "Epoch 26/200\n",
      "29325/29325 [==============================] - 16s 541us/step - loss: 0.9431 - acc: 0.6845\n",
      "Epoch 27/200\n",
      "29325/29325 [==============================] - 16s 541us/step - loss: 0.8979 - acc: 0.7002\n",
      "Epoch 28/200\n",
      "29325/29325 [==============================] - 16s 541us/step - loss: 0.8639 - acc: 0.7104\n",
      "Epoch 29/200\n",
      "29325/29325 [==============================] - 16s 542us/step - loss: 0.8253 - acc: 0.7240\n",
      "Epoch 30/200\n",
      "29325/29325 [==============================] - 16s 542us/step - loss: 0.7887 - acc: 0.7318\n",
      "Epoch 31/200\n",
      "29325/29325 [==============================] - 16s 542us/step - loss: 0.7647 - acc: 0.7428\n",
      "Epoch 32/200\n",
      "29325/29325 [==============================] - 16s 542us/step - loss: 0.7328 - acc: 0.7511\n",
      "Epoch 33/200\n",
      "29325/29325 [==============================] - 16s 546us/step - loss: 0.7015 - acc: 0.7610\n",
      "Epoch 34/200\n",
      "29325/29325 [==============================] - 16s 542us/step - loss: 0.6758 - acc: 0.7685\n",
      "Epoch 35/200\n",
      "29325/29325 [==============================] - 16s 541us/step - loss: 0.6623 - acc: 0.7751\n",
      "Epoch 36/200\n",
      "29325/29325 [==============================] - 16s 541us/step - loss: 0.6292 - acc: 0.7857\n",
      "Epoch 37/200\n",
      "29325/29325 [==============================] - 16s 542us/step - loss: 0.5957 - acc: 0.7952\n",
      "Epoch 38/200\n",
      "29325/29325 [==============================] - 16s 542us/step - loss: 0.5823 - acc: 0.8040\n",
      "Epoch 39/200\n",
      "29325/29325 [==============================] - 16s 542us/step - loss: 0.5705 - acc: 0.8046\n",
      "Epoch 40/200\n",
      "29325/29325 [==============================] - 16s 542us/step - loss: 0.5423 - acc: 0.8160\n",
      "Epoch 41/200\n",
      "29325/29325 [==============================] - 16s 541us/step - loss: 0.5199 - acc: 0.8225\n",
      "Epoch 42/200\n",
      "29325/29325 [==============================] - 16s 542us/step - loss: 0.5048 - acc: 0.8265\n",
      "Epoch 43/200\n",
      "29325/29325 [==============================] - 16s 541us/step - loss: 0.4785 - acc: 0.8367\n",
      "Epoch 44/200\n",
      "29325/29325 [==============================] - 16s 542us/step - loss: 0.4681 - acc: 0.8389\n",
      "Epoch 45/200\n",
      "29325/29325 [==============================] - 16s 542us/step - loss: 0.4552 - acc: 0.8436\n",
      "Epoch 46/200\n",
      "29325/29325 [==============================] - 16s 542us/step - loss: 0.4473 - acc: 0.8448\n",
      "Epoch 47/200\n",
      "29325/29325 [==============================] - 16s 542us/step - loss: 0.4302 - acc: 0.8507\n",
      "Epoch 48/200\n",
      "29325/29325 [==============================] - 16s 542us/step - loss: 0.4092 - acc: 0.8594\n",
      "Epoch 49/200\n",
      "29325/29325 [==============================] - 16s 542us/step - loss: 0.3963 - acc: 0.8619\n",
      "Epoch 50/200\n",
      "29325/29325 [==============================] - 16s 542us/step - loss: 0.3961 - acc: 0.8645\n",
      "Epoch 51/200\n",
      "29325/29325 [==============================] - 16s 542us/step - loss: 0.3821 - acc: 0.8683\n",
      "Epoch 52/200\n",
      "29325/29325 [==============================] - 16s 543us/step - loss: 0.3706 - acc: 0.8738\n",
      "Epoch 53/200\n",
      "29325/29325 [==============================] - 16s 542us/step - loss: 0.3629 - acc: 0.8736\n",
      "Epoch 54/200\n",
      "29325/29325 [==============================] - 16s 549us/step - loss: 0.3600 - acc: 0.8767\n",
      "Epoch 55/200\n",
      "29325/29325 [==============================] - 16s 543us/step - loss: 0.3293 - acc: 0.8861\n",
      "Epoch 56/200\n",
      "29325/29325 [==============================] - 16s 542us/step - loss: 0.3333 - acc: 0.8856\n",
      "Epoch 57/200\n",
      "29325/29325 [==============================] - 16s 542us/step - loss: 0.3172 - acc: 0.8906\n",
      "Epoch 58/200\n",
      "29325/29325 [==============================] - 16s 541us/step - loss: 0.3362 - acc: 0.8853\n",
      "Epoch 59/200\n",
      "29325/29325 [==============================] - 16s 542us/step - loss: 0.3120 - acc: 0.8933\n",
      "Epoch 60/200\n",
      "29325/29325 [==============================] - 16s 541us/step - loss: 0.3044 - acc: 0.8944\n",
      "Epoch 61/200\n",
      "29325/29325 [==============================] - 16s 541us/step - loss: 0.3182 - acc: 0.8906\n",
      "Epoch 62/200\n",
      "29325/29325 [==============================] - 16s 541us/step - loss: 0.3060 - acc: 0.8960\n",
      "Epoch 63/200\n",
      "29325/29325 [==============================] - 16s 542us/step - loss: 0.2857 - acc: 0.9017\n",
      "Epoch 64/200\n",
      "29325/29325 [==============================] - 16s 542us/step - loss: 0.2785 - acc: 0.9048\n",
      "Epoch 65/200\n",
      "29325/29325 [==============================] - 16s 542us/step - loss: 0.2850 - acc: 0.9028\n",
      "Epoch 66/200\n",
      "29325/29325 [==============================] - 16s 542us/step - loss: 0.2815 - acc: 0.9029\n",
      "Epoch 67/200\n",
      "29325/29325 [==============================] - 16s 541us/step - loss: 0.2661 - acc: 0.9081\n",
      "Epoch 68/200\n",
      "29325/29325 [==============================] - 16s 542us/step - loss: 0.2615 - acc: 0.9123\n",
      "Epoch 69/200\n",
      "29325/29325 [==============================] - 16s 542us/step - loss: 0.2599 - acc: 0.9116\n",
      "Epoch 70/200\n",
      "29325/29325 [==============================] - 16s 541us/step - loss: 0.2659 - acc: 0.9084\n",
      "Epoch 71/200\n",
      "29325/29325 [==============================] - 16s 544us/step - loss: 0.2636 - acc: 0.9107\n",
      "Epoch 72/200\n",
      "29325/29325 [==============================] - 16s 542us/step - loss: 0.2521 - acc: 0.9127\n",
      "Epoch 73/200\n",
      "29325/29325 [==============================] - 16s 542us/step - loss: 0.2367 - acc: 0.9179\n",
      "Epoch 74/200\n",
      "29325/29325 [==============================] - 16s 542us/step - loss: 0.2307 - acc: 0.9214\n",
      "Epoch 75/200\n",
      "29325/29325 [==============================] - 16s 542us/step - loss: 0.2424 - acc: 0.9169\n",
      "Epoch 76/200\n",
      "29325/29325 [==============================] - 16s 542us/step - loss: 0.2353 - acc: 0.9194\n",
      "Epoch 77/200\n",
      "29325/29325 [==============================] - 16s 541us/step - loss: 0.2450 - acc: 0.9159\n",
      "Epoch 78/200\n",
      "29325/29325 [==============================] - 16s 542us/step - loss: 0.2316 - acc: 0.9218\n",
      "Epoch 79/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29325/29325 [==============================] - 16s 541us/step - loss: 0.2280 - acc: 0.9218\n",
      "Epoch 80/200\n",
      "29325/29325 [==============================] - 16s 541us/step - loss: 0.2201 - acc: 0.9232\n",
      "Epoch 81/200\n",
      "29325/29325 [==============================] - 16s 542us/step - loss: 0.2211 - acc: 0.9251\n",
      "Epoch 82/200\n",
      "29325/29325 [==============================] - 16s 541us/step - loss: 0.2271 - acc: 0.9230\n",
      "Epoch 83/200\n",
      "29325/29325 [==============================] - 16s 542us/step - loss: 0.2227 - acc: 0.9231\n",
      "Epoch 84/200\n",
      "29325/29325 [==============================] - 16s 542us/step - loss: 0.2126 - acc: 0.9284\n",
      "Epoch 85/200\n",
      "29325/29325 [==============================] - 16s 541us/step - loss: 0.2183 - acc: 0.9264\n",
      "Epoch 86/200\n",
      "29325/29325 [==============================] - 16s 541us/step - loss: 0.2027 - acc: 0.9294\n",
      "Epoch 87/200\n",
      "29325/29325 [==============================] - 16s 541us/step - loss: 0.2064 - acc: 0.9289\n",
      "Epoch 88/200\n",
      "29325/29325 [==============================] - 16s 541us/step - loss: 0.2137 - acc: 0.9286\n",
      "Epoch 89/200\n",
      "29325/29325 [==============================] - 16s 541us/step - loss: 0.2150 - acc: 0.9284\n",
      "Epoch 90/200\n",
      "29325/29325 [==============================] - 16s 542us/step - loss: 0.1975 - acc: 0.9337\n",
      "Epoch 91/200\n",
      "29325/29325 [==============================] - 16s 542us/step - loss: 0.1867 - acc: 0.9357\n",
      "Epoch 92/200\n",
      "29325/29325 [==============================] - 16s 542us/step - loss: 0.1903 - acc: 0.9366\n",
      "Epoch 93/200\n",
      "29325/29325 [==============================] - 16s 541us/step - loss: 0.2085 - acc: 0.9302\n",
      "Epoch 94/200\n",
      "29325/29325 [==============================] - 16s 541us/step - loss: 0.1952 - acc: 0.9331\n",
      "Epoch 95/200\n",
      "29325/29325 [==============================] - 16s 542us/step - loss: 0.1890 - acc: 0.9344\n",
      "Epoch 96/200\n",
      "29325/29325 [==============================] - 16s 542us/step - loss: 0.1784 - acc: 0.9410\n",
      "Epoch 97/200\n",
      "29325/29325 [==============================] - 16s 541us/step - loss: 0.1850 - acc: 0.9377\n",
      "Epoch 98/200\n",
      "29325/29325 [==============================] - 16s 542us/step - loss: 0.1785 - acc: 0.9396\n",
      "Epoch 99/200\n",
      "29325/29325 [==============================] - 16s 541us/step - loss: 0.1860 - acc: 0.9366\n",
      "Epoch 100/200\n",
      "29325/29325 [==============================] - 16s 541us/step - loss: 0.1851 - acc: 0.9377\n",
      "Epoch 101/200\n",
      "29325/29325 [==============================] - 16s 541us/step - loss: 0.1794 - acc: 0.9393\n",
      "Epoch 102/200\n",
      "29325/29325 [==============================] - 16s 541us/step - loss: 0.1631 - acc: 0.9449\n",
      "Epoch 103/200\n",
      "29325/29325 [==============================] - 16s 542us/step - loss: 0.1826 - acc: 0.9399\n",
      "Epoch 104/200\n",
      "29325/29325 [==============================] - 16s 542us/step - loss: 0.1922 - acc: 0.9357\n",
      "Epoch 105/200\n",
      "29325/29325 [==============================] - 16s 541us/step - loss: 0.1646 - acc: 0.9436\n",
      "Epoch 106/200\n",
      "29325/29325 [==============================] - 16s 542us/step - loss: 0.1740 - acc: 0.9395\n",
      "Epoch 107/200\n",
      "29325/29325 [==============================] - 16s 541us/step - loss: 0.1721 - acc: 0.9411\n",
      "Epoch 108/200\n",
      "29325/29325 [==============================] - 16s 542us/step - loss: 0.1772 - acc: 0.9391\n",
      "Epoch 109/200\n",
      "29325/29325 [==============================] - 16s 546us/step - loss: 0.1684 - acc: 0.9438\n",
      "Epoch 110/200\n",
      "29325/29325 [==============================] - 16s 542us/step - loss: 0.1634 - acc: 0.9450\n",
      "Epoch 111/200\n",
      "29325/29325 [==============================] - 16s 542us/step - loss: 0.1737 - acc: 0.9420\n",
      "Epoch 112/200\n",
      "29325/29325 [==============================] - 16s 542us/step - loss: 0.1750 - acc: 0.9422\n",
      "Epoch 113/200\n",
      "29325/29325 [==============================] - 16s 542us/step - loss: 0.1562 - acc: 0.9461\n",
      "Epoch 114/200\n",
      "29325/29325 [==============================] - 16s 541us/step - loss: 0.1640 - acc: 0.9454\n",
      "Epoch 115/200\n",
      "29325/29325 [==============================] - 16s 541us/step - loss: 0.1509 - acc: 0.9483\n",
      "Epoch 116/200\n",
      "29325/29325 [==============================] - 16s 541us/step - loss: 0.1592 - acc: 0.9466\n",
      "Epoch 117/200\n",
      "29325/29325 [==============================] - 16s 542us/step - loss: 0.1600 - acc: 0.9477\n",
      "Epoch 118/200\n",
      "29325/29325 [==============================] - 16s 541us/step - loss: 0.1519 - acc: 0.9491\n",
      "Epoch 119/200\n",
      "29325/29325 [==============================] - 16s 542us/step - loss: 0.1491 - acc: 0.9496\n",
      "Epoch 120/200\n",
      "29325/29325 [==============================] - 16s 541us/step - loss: 0.1637 - acc: 0.9452\n",
      "Epoch 121/200\n",
      "29325/29325 [==============================] - 16s 542us/step - loss: 0.1651 - acc: 0.9450\n",
      "Epoch 122/200\n",
      "29325/29325 [==============================] - 16s 541us/step - loss: 0.1589 - acc: 0.9468\n",
      "Epoch 123/200\n",
      "29325/29325 [==============================] - 16s 542us/step - loss: 0.1596 - acc: 0.9465\n",
      "Epoch 124/200\n",
      "29325/29325 [==============================] - 16s 542us/step - loss: 0.1484 - acc: 0.9497\n",
      "Epoch 125/200\n",
      "29325/29325 [==============================] - 16s 542us/step - loss: 0.1489 - acc: 0.9502\n",
      "Epoch 126/200\n",
      "29325/29325 [==============================] - 16s 541us/step - loss: 0.1410 - acc: 0.9521\n",
      "Epoch 127/200\n",
      "29325/29325 [==============================] - 16s 542us/step - loss: 0.1501 - acc: 0.9505\n",
      "Epoch 128/200\n",
      "29325/29325 [==============================] - 16s 543us/step - loss: 0.1550 - acc: 0.9494\n",
      "Epoch 129/200\n",
      "29325/29325 [==============================] - 16s 542us/step - loss: 0.1420 - acc: 0.9512\n",
      "Epoch 130/200\n",
      "29325/29325 [==============================] - 16s 541us/step - loss: 0.1508 - acc: 0.9500\n",
      "Epoch 131/200\n",
      "29325/29325 [==============================] - 16s 542us/step - loss: 0.1343 - acc: 0.9556\n",
      "Epoch 132/200\n",
      "29325/29325 [==============================] - 16s 542us/step - loss: 0.1514 - acc: 0.9501\n",
      "Epoch 133/200\n",
      "29325/29325 [==============================] - 16s 541us/step - loss: 0.1406 - acc: 0.9518\n",
      "Epoch 134/200\n",
      "29325/29325 [==============================] - 16s 543us/step - loss: 0.1300 - acc: 0.9555\n",
      "Epoch 135/200\n",
      "29325/29325 [==============================] - 16s 541us/step - loss: 0.1459 - acc: 0.9522\n",
      "Epoch 136/200\n",
      "29325/29325 [==============================] - 16s 541us/step - loss: 0.1407 - acc: 0.9533\n",
      "Epoch 137/200\n",
      "29325/29325 [==============================] - 16s 542us/step - loss: 0.1354 - acc: 0.9546\n",
      "Epoch 138/200\n",
      "29325/29325 [==============================] - 16s 541us/step - loss: 0.1461 - acc: 0.9518\n",
      "Epoch 139/200\n",
      "29325/29325 [==============================] - 16s 541us/step - loss: 0.1493 - acc: 0.9489\n",
      "Epoch 140/200\n",
      "29325/29325 [==============================] - 16s 542us/step - loss: 0.1308 - acc: 0.9556\n",
      "Epoch 141/200\n",
      "29325/29325 [==============================] - 16s 541us/step - loss: 0.1278 - acc: 0.9576\n",
      "Epoch 142/200\n",
      "29325/29325 [==============================] - 16s 542us/step - loss: 0.1286 - acc: 0.9566\n",
      "Epoch 143/200\n",
      "29325/29325 [==============================] - 16s 541us/step - loss: 0.1449 - acc: 0.9515\n",
      "Epoch 144/200\n",
      "29325/29325 [==============================] - 16s 541us/step - loss: 0.1308 - acc: 0.9550\n",
      "Epoch 145/200\n",
      "29325/29325 [==============================] - 16s 541us/step - loss: 0.1235 - acc: 0.9596\n",
      "Epoch 146/200\n",
      "29325/29325 [==============================] - 16s 543us/step - loss: 0.1367 - acc: 0.9560\n",
      "Epoch 147/200\n",
      "29325/29325 [==============================] - 16s 542us/step - loss: 0.1387 - acc: 0.9541\n",
      "Epoch 148/200\n",
      "29325/29325 [==============================] - 16s 541us/step - loss: 0.1333 - acc: 0.9567\n",
      "Epoch 149/200\n",
      "29325/29325 [==============================] - 16s 541us/step - loss: 0.1316 - acc: 0.9564\n",
      "Epoch 150/200\n",
      "29325/29325 [==============================] - 16s 542us/step - loss: 0.1341 - acc: 0.9552\n",
      "Epoch 151/200\n",
      "29325/29325 [==============================] - 16s 542us/step - loss: 0.1256 - acc: 0.9582\n",
      "Epoch 152/200\n",
      "29325/29325 [==============================] - 16s 542us/step - loss: 0.1281 - acc: 0.9580\n",
      "Epoch 153/200\n",
      "29325/29325 [==============================] - 16s 541us/step - loss: 0.1317 - acc: 0.9556\n",
      "Epoch 154/200\n",
      "29325/29325 [==============================] - 16s 541us/step - loss: 0.1278 - acc: 0.9559\n",
      "Epoch 155/200\n",
      "29325/29325 [==============================] - 16s 541us/step - loss: 0.1296 - acc: 0.9568\n",
      "Epoch 156/200\n",
      "29325/29325 [==============================] - 16s 541us/step - loss: 0.1286 - acc: 0.9571\n",
      "Epoch 157/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29325/29325 [==============================] - 16s 542us/step - loss: 0.1205 - acc: 0.9596\n",
      "Epoch 158/200\n",
      "29325/29325 [==============================] - 16s 541us/step - loss: 0.1194 - acc: 0.9603\n",
      "Epoch 159/200\n",
      "29325/29325 [==============================] - 16s 541us/step - loss: 0.1152 - acc: 0.9615\n",
      "Epoch 160/200\n",
      "29325/29325 [==============================] - 16s 542us/step - loss: 0.1302 - acc: 0.9571\n",
      "Epoch 161/200\n",
      "29325/29325 [==============================] - 16s 541us/step - loss: 0.1295 - acc: 0.9586\n",
      "Epoch 162/200\n",
      "29325/29325 [==============================] - 16s 541us/step - loss: 0.1250 - acc: 0.9586\n",
      "Epoch 163/200\n",
      "29325/29325 [==============================] - 16s 542us/step - loss: 0.1277 - acc: 0.9577\n",
      "Epoch 164/200\n",
      "29325/29325 [==============================] - 16s 542us/step - loss: 0.1065 - acc: 0.9648\n",
      "Epoch 165/200\n",
      "29325/29325 [==============================] - 16s 543us/step - loss: 0.1176 - acc: 0.9615\n",
      "Epoch 166/200\n",
      "29325/29325 [==============================] - 16s 541us/step - loss: 0.1227 - acc: 0.9598\n",
      "Epoch 167/200\n",
      "29325/29325 [==============================] - 16s 544us/step - loss: 0.1195 - acc: 0.9608\n",
      "Epoch 168/200\n",
      "29325/29325 [==============================] - 16s 542us/step - loss: 0.1181 - acc: 0.9613\n",
      "Epoch 169/200\n",
      "29325/29325 [==============================] - 16s 541us/step - loss: 0.1177 - acc: 0.9613\n",
      "Epoch 170/200\n",
      "29325/29325 [==============================] - 16s 542us/step - loss: 0.1199 - acc: 0.9609\n",
      "Epoch 171/200\n",
      "29325/29325 [==============================] - 16s 541us/step - loss: 0.1191 - acc: 0.9605\n",
      "Epoch 172/200\n",
      "29325/29325 [==============================] - 16s 542us/step - loss: 0.1168 - acc: 0.9613\n",
      "Epoch 173/200\n",
      "29325/29325 [==============================] - 16s 541us/step - loss: 0.1295 - acc: 0.9578\n",
      "Epoch 174/200\n",
      "29325/29325 [==============================] - 16s 542us/step - loss: 0.1270 - acc: 0.9588\n",
      "Epoch 175/200\n",
      "29325/29325 [==============================] - 16s 541us/step - loss: 0.1113 - acc: 0.9640\n",
      "Epoch 176/200\n",
      "29325/29325 [==============================] - 16s 542us/step - loss: 0.1023 - acc: 0.9650\n",
      "Epoch 177/200\n",
      "29325/29325 [==============================] - 16s 542us/step - loss: 0.1020 - acc: 0.9657\n",
      "Epoch 178/200\n",
      "29325/29325 [==============================] - 16s 542us/step - loss: 0.1251 - acc: 0.9589\n",
      "Epoch 179/200\n",
      "29325/29325 [==============================] - 16s 541us/step - loss: 0.1172 - acc: 0.9604\n",
      "Epoch 180/200\n",
      "29325/29325 [==============================] - 16s 541us/step - loss: 0.1171 - acc: 0.9598\n",
      "Epoch 181/200\n",
      "29325/29325 [==============================] - 16s 542us/step - loss: 0.1173 - acc: 0.9613\n",
      "Epoch 182/200\n",
      "29325/29325 [==============================] - 16s 541us/step - loss: 0.1068 - acc: 0.9650\n",
      "Epoch 183/200\n",
      "29325/29325 [==============================] - 16s 541us/step - loss: 0.0977 - acc: 0.9678\n",
      "Epoch 184/200\n",
      "29325/29325 [==============================] - 16s 545us/step - loss: 0.1027 - acc: 0.9658\n",
      "Epoch 185/200\n",
      "29325/29325 [==============================] - 16s 542us/step - loss: 0.0996 - acc: 0.9668\n",
      "Epoch 186/200\n",
      "29325/29325 [==============================] - 16s 541us/step - loss: 0.1173 - acc: 0.9609\n",
      "Epoch 187/200\n",
      "29325/29325 [==============================] - 16s 542us/step - loss: 0.1317 - acc: 0.9570\n",
      "Epoch 188/200\n",
      "29325/29325 [==============================] - 16s 542us/step - loss: 0.1130 - acc: 0.9623\n",
      "Epoch 189/200\n",
      "29325/29325 [==============================] - 16s 541us/step - loss: 0.1103 - acc: 0.9639\n",
      "Epoch 190/200\n",
      "29325/29325 [==============================] - 16s 541us/step - loss: 0.0973 - acc: 0.9678\n",
      "Epoch 191/200\n",
      "29325/29325 [==============================] - 16s 541us/step - loss: 0.1104 - acc: 0.9624\n",
      "Epoch 192/200\n",
      "29325/29325 [==============================] - 16s 541us/step - loss: 0.1083 - acc: 0.9644\n",
      "Epoch 193/200\n",
      "29325/29325 [==============================] - 16s 542us/step - loss: 0.1138 - acc: 0.9629\n",
      "Epoch 194/200\n",
      "29325/29325 [==============================] - 16s 542us/step - loss: 0.1006 - acc: 0.9666\n",
      "Epoch 195/200\n",
      "29325/29325 [==============================] - 16s 542us/step - loss: 0.0941 - acc: 0.9694\n",
      "Epoch 196/200\n",
      "29325/29325 [==============================] - 16s 542us/step - loss: 0.0860 - acc: 0.9716\n",
      "Epoch 197/200\n",
      "29325/29325 [==============================] - 16s 541us/step - loss: 0.1166 - acc: 0.9617\n",
      "Epoch 198/200\n",
      "29325/29325 [==============================] - 16s 541us/step - loss: 0.1117 - acc: 0.9628\n",
      "Epoch 199/200\n",
      "29325/29325 [==============================] - 16s 542us/step - loss: 0.1242 - acc: 0.9587\n",
      "Epoch 200/200\n",
      "29325/29325 [==============================] - 16s 542us/step - loss: 0.1031 - acc: 0.9660\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f619c506518>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.fit(x, y, batch_size=64, epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nmeyer/Documents/cs155/project3/tensorflow/lib64/python3.6/site-packages/ipykernel_launcher.py:4: RuntimeWarning: divide by zero encountered in log\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shall i compare thee to a summer's day?\n",
      "the ubray wo be my sett ned i lies thee,\n",
      "  tit fres of y uthing she love nature grow.\n",
      "  they should fround your mands and lain,\n",
      "and shall i to heart in my sight,\n",
      "and thought your love, i sin my for my soul,\n",
      "for eashed that i still tist arg of the bearth to see.\n",
      "hou in the cheer of thate of life i say thee true,\n",
      "which the love some but thine i than a feend,\n",
      "now bars of me set my gaver love deemone,\n",
      "duse when thee her ly spe my love to my ell:\n",
      "so exe their grown they me turne your upfery,\n",
      "now be the can the sleant, and your fart redenged:\n",
      "bet of beaute's dost come my fander lie.\n",
      "and no from my fander that but inwife ree.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#seed = \"shall i compare thee to a summer's day?\\nthou art more lovely\"\n",
    "seed = \"shall i compare thee to a summer's day?\\n\"\n",
    "generated = \"\" + seed\n",
    "\n",
    "# Generates 14 lines of poem (More efficient)\n",
    "\n",
    "x_pred = np.zeros((1, char_length - 1, len(chars)))\n",
    "for t, char in enumerate(seed):\n",
    "    x_pred[0, t, char_indices[char]] = 1.\n",
    "preds = model3.predict(x_pred, verbose=0)[0]\n",
    "\n",
    "for i in range(14):\n",
    "    while True:\n",
    "        next_index = sample(preds, temperature=0.5)\n",
    "        next_char = indices_char[next_index]\n",
    "        \n",
    "        x_pred[0, :-1, :] = x_pred[:, 1:, :]\n",
    "        x_pred[0, -1, :] = np.zeros(len(chars))\n",
    "        x_pred[0, -1, char_indices[next_char]] = 1.\n",
    "        preds = model3.predict(x_pred, verbose=0)[0]\n",
    "        \n",
    "        generated += next_char\n",
    "        if next_char == \"\\n\":\n",
    "            #print(generated)\n",
    "            #generated = \"\"\n",
    "            break\n",
    "        #generated += next_char\n",
    "        \n",
    "\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "29325/29325 [==============================] - 17s 570us/step - loss: 0.0930 - acc: 0.9682\n",
      "Epoch 2/200\n",
      "29325/29325 [==============================] - 17s 566us/step - loss: 0.1023 - acc: 0.9670\n",
      "Epoch 3/200\n",
      "29325/29325 [==============================] - 17s 564us/step - loss: 0.0999 - acc: 0.9673\n",
      "Epoch 4/200\n",
      "29325/29325 [==============================] - 17s 565us/step - loss: 0.0948 - acc: 0.9677\n",
      "Epoch 5/200\n",
      "29325/29325 [==============================] - 16s 561us/step - loss: 0.1177 - acc: 0.9609\n",
      "Epoch 6/200\n",
      "29325/29325 [==============================] - 17s 566us/step - loss: 0.1050 - acc: 0.9655\n",
      "Epoch 7/200\n",
      "29325/29325 [==============================] - 16s 562us/step - loss: 0.0980 - acc: 0.9670\n",
      "Epoch 8/200\n",
      "29325/29325 [==============================] - 17s 563us/step - loss: 0.0915 - acc: 0.9697\n",
      "Epoch 9/200\n",
      "29325/29325 [==============================] - 17s 563us/step - loss: 0.0906 - acc: 0.9706\n",
      "Epoch 10/200\n",
      "29325/29325 [==============================] - 17s 564us/step - loss: 0.0997 - acc: 0.9670\n",
      "Epoch 11/200\n",
      "29325/29325 [==============================] - 17s 564us/step - loss: 0.1155 - acc: 0.9627\n",
      "Epoch 12/200\n",
      "29325/29325 [==============================] - 17s 564us/step - loss: 0.1038 - acc: 0.9664\n",
      "Epoch 13/200\n",
      "29325/29325 [==============================] - 17s 564us/step - loss: 0.1058 - acc: 0.9653\n",
      "Epoch 14/200\n",
      "29325/29325 [==============================] - 17s 564us/step - loss: 0.1010 - acc: 0.9670\n",
      "Epoch 15/200\n",
      "29325/29325 [==============================] - 17s 564us/step - loss: 0.0989 - acc: 0.9670\n",
      "Epoch 16/200\n",
      "29325/29325 [==============================] - 17s 563us/step - loss: 0.0981 - acc: 0.9687\n",
      "Epoch 17/200\n",
      "29325/29325 [==============================] - 17s 563us/step - loss: 0.0946 - acc: 0.9689\n",
      "Epoch 18/200\n",
      "29325/29325 [==============================] - 17s 564us/step - loss: 0.0923 - acc: 0.9687\n",
      "Epoch 19/200\n",
      "29325/29325 [==============================] - 17s 564us/step - loss: 0.0928 - acc: 0.9684\n",
      "Epoch 20/200\n",
      "29325/29325 [==============================] - 17s 565us/step - loss: 0.0836 - acc: 0.9720\n",
      "Epoch 21/200\n",
      "29325/29325 [==============================] - 17s 565us/step - loss: 0.1056 - acc: 0.9655\n",
      "Epoch 22/200\n",
      "29325/29325 [==============================] - 17s 565us/step - loss: 0.1085 - acc: 0.9635\n",
      "Epoch 23/200\n",
      "29325/29325 [==============================] - 17s 565us/step - loss: 0.1031 - acc: 0.9661\n",
      "Epoch 24/200\n",
      "29325/29325 [==============================] - 17s 571us/step - loss: 0.0958 - acc: 0.9685\n",
      "Epoch 25/200\n",
      "29325/29325 [==============================] - 17s 565us/step - loss: 0.0886 - acc: 0.9714\n",
      "Epoch 26/200\n",
      "29325/29325 [==============================] - 17s 567us/step - loss: 0.0801 - acc: 0.9731\n",
      "Epoch 27/200\n",
      "29325/29325 [==============================] - 17s 564us/step - loss: 0.0937 - acc: 0.9699\n",
      "Epoch 28/200\n",
      "29325/29325 [==============================] - 17s 565us/step - loss: 0.1084 - acc: 0.9645\n",
      "Epoch 29/200\n",
      "29325/29325 [==============================] - 17s 565us/step - loss: 0.0861 - acc: 0.9720\n",
      "Epoch 30/200\n",
      "29325/29325 [==============================] - 17s 567us/step - loss: 0.0847 - acc: 0.9716\n",
      "Epoch 31/200\n",
      "29325/29325 [==============================] - 17s 565us/step - loss: 0.1104 - acc: 0.9657\n",
      "Epoch 32/200\n",
      "29325/29325 [==============================] - 17s 565us/step - loss: 0.0905 - acc: 0.9703\n",
      "Epoch 33/200\n",
      "29325/29325 [==============================] - 17s 564us/step - loss: 0.0875 - acc: 0.9707\n",
      "Epoch 34/200\n",
      "29325/29325 [==============================] - 17s 565us/step - loss: 0.0929 - acc: 0.9699\n",
      "Epoch 35/200\n",
      "29325/29325 [==============================] - 17s 565us/step - loss: 0.0909 - acc: 0.9704\n",
      "Epoch 36/200\n",
      "29325/29325 [==============================] - 17s 565us/step - loss: 0.0861 - acc: 0.9717\n",
      "Epoch 37/200\n",
      "29325/29325 [==============================] - 17s 564us/step - loss: 0.0957 - acc: 0.9685\n",
      "Epoch 38/200\n",
      "29325/29325 [==============================] - 17s 565us/step - loss: 0.0946 - acc: 0.9701\n",
      "Epoch 39/200\n",
      "29325/29325 [==============================] - 17s 566us/step - loss: 0.0903 - acc: 0.9700\n",
      "Epoch 40/200\n",
      "29325/29325 [==============================] - 17s 568us/step - loss: 0.0886 - acc: 0.9717\n",
      "Epoch 41/200\n",
      "29325/29325 [==============================] - 17s 566us/step - loss: 0.0823 - acc: 0.9732\n",
      "Epoch 42/200\n",
      "29325/29325 [==============================] - 17s 567us/step - loss: 0.0906 - acc: 0.9705\n",
      "Epoch 43/200\n",
      "29325/29325 [==============================] - 17s 565us/step - loss: 0.0984 - acc: 0.9672\n",
      "Epoch 44/200\n",
      "29325/29325 [==============================] - 17s 566us/step - loss: 0.0903 - acc: 0.9711\n",
      "Epoch 45/200\n",
      "29325/29325 [==============================] - 17s 566us/step - loss: 0.0783 - acc: 0.9739\n",
      "Epoch 46/200\n",
      "29325/29325 [==============================] - 17s 568us/step - loss: 0.0765 - acc: 0.9743\n",
      "Epoch 47/200\n",
      "29325/29325 [==============================] - 17s 568us/step - loss: 0.0804 - acc: 0.9733\n",
      "Epoch 48/200\n",
      "29325/29325 [==============================] - 17s 565us/step - loss: 0.0909 - acc: 0.9714\n",
      "Epoch 49/200\n",
      "29325/29325 [==============================] - 17s 568us/step - loss: 0.0949 - acc: 0.9689\n",
      "Epoch 50/200\n",
      "29325/29325 [==============================] - 17s 566us/step - loss: 0.0993 - acc: 0.9686\n",
      "Epoch 51/200\n",
      "29325/29325 [==============================] - 17s 566us/step - loss: 0.0793 - acc: 0.9735\n",
      "Epoch 52/200\n",
      "29325/29325 [==============================] - 17s 568us/step - loss: 0.0747 - acc: 0.9758\n",
      "Epoch 53/200\n",
      "29325/29325 [==============================] - 17s 567us/step - loss: 0.0791 - acc: 0.9735\n",
      "Epoch 54/200\n",
      "29325/29325 [==============================] - 17s 569us/step - loss: 0.0776 - acc: 0.9744\n",
      "Epoch 55/200\n",
      "29325/29325 [==============================] - 17s 570us/step - loss: 0.0812 - acc: 0.9739\n",
      "Epoch 56/200\n",
      "29325/29325 [==============================] - 17s 567us/step - loss: 0.0829 - acc: 0.9728\n",
      "Epoch 57/200\n",
      "29325/29325 [==============================] - 17s 569us/step - loss: 0.0919 - acc: 0.9703\n",
      "Epoch 58/200\n",
      "29325/29325 [==============================] - 17s 567us/step - loss: 0.0863 - acc: 0.9704\n",
      "Epoch 59/200\n",
      "29325/29325 [==============================] - 17s 571us/step - loss: 0.0786 - acc: 0.9729\n",
      "Epoch 60/200\n",
      "29325/29325 [==============================] - 17s 570us/step - loss: 0.0860 - acc: 0.9708\n",
      "Epoch 61/200\n",
      "29325/29325 [==============================] - 17s 567us/step - loss: 0.0816 - acc: 0.9732\n",
      "Epoch 62/200\n",
      "29325/29325 [==============================] - 17s 568us/step - loss: 0.0762 - acc: 0.9753\n",
      "Epoch 63/200\n",
      "29325/29325 [==============================] - 17s 571us/step - loss: 0.0827 - acc: 0.9730\n",
      "Epoch 64/200\n",
      "29325/29325 [==============================] - 17s 568us/step - loss: 0.0819 - acc: 0.9737\n",
      "Epoch 65/200\n",
      "29325/29325 [==============================] - 17s 569us/step - loss: 0.0949 - acc: 0.9692\n",
      "Epoch 66/200\n",
      "29325/29325 [==============================] - 17s 570us/step - loss: 0.0845 - acc: 0.9733\n",
      "Epoch 67/200\n",
      "29325/29325 [==============================] - 17s 570us/step - loss: 0.0801 - acc: 0.9745\n",
      "Epoch 68/200\n",
      "29325/29325 [==============================] - 17s 571us/step - loss: 0.0752 - acc: 0.9752\n",
      "Epoch 69/200\n",
      "29325/29325 [==============================] - 17s 571us/step - loss: 0.0718 - acc: 0.9770\n",
      "Epoch 70/200\n",
      "29325/29325 [==============================] - 17s 570us/step - loss: 0.0900 - acc: 0.9713\n",
      "Epoch 71/200\n",
      "29325/29325 [==============================] - 17s 572us/step - loss: 0.0738 - acc: 0.9770\n",
      "Epoch 72/200\n",
      "29325/29325 [==============================] - 17s 570us/step - loss: 0.0794 - acc: 0.9749\n",
      "Epoch 73/200\n",
      "29325/29325 [==============================] - 17s 569us/step - loss: 0.0932 - acc: 0.9683\n",
      "Epoch 74/200\n",
      "29325/29325 [==============================] - 17s 568us/step - loss: 0.0811 - acc: 0.9728\n",
      "Epoch 75/200\n",
      "29325/29325 [==============================] - 17s 572us/step - loss: 0.0745 - acc: 0.9746\n",
      "Epoch 76/200\n",
      "29325/29325 [==============================] - 17s 573us/step - loss: 0.0769 - acc: 0.9743\n",
      "Epoch 77/200\n",
      "29325/29325 [==============================] - 17s 581us/step - loss: 0.0741 - acc: 0.9757\n",
      "Epoch 78/200\n",
      "29325/29325 [==============================] - 17s 584us/step - loss: 0.0737 - acc: 0.9756\n",
      "Epoch 79/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29325/29325 [==============================] - 17s 594us/step - loss: 0.0823 - acc: 0.9729\n",
      "Epoch 80/200\n",
      "29325/29325 [==============================] - 17s 591us/step - loss: 0.0728 - acc: 0.9763\n",
      "Epoch 81/200\n",
      "29325/29325 [==============================] - 18s 602us/step - loss: 0.0759 - acc: 0.9762\n",
      "Epoch 82/200\n",
      "29325/29325 [==============================] - 17s 597us/step - loss: 0.0844 - acc: 0.9727\n",
      "Epoch 83/200\n",
      "29325/29325 [==============================] - 17s 569us/step - loss: 0.0782 - acc: 0.9730\n",
      "Epoch 84/200\n",
      "29325/29325 [==============================] - 19s 646us/step - loss: 0.0807 - acc: 0.9720\n",
      "Epoch 85/200\n",
      "29325/29325 [==============================] - 16s 556us/step - loss: 0.0764 - acc: 0.9745\n",
      "Epoch 86/200\n",
      "29325/29325 [==============================] - 16s 557us/step - loss: 0.0716 - acc: 0.9762\n",
      "Epoch 87/200\n",
      "29325/29325 [==============================] - 17s 586us/step - loss: 0.0781 - acc: 0.9744\n",
      "Epoch 88/200\n",
      "29325/29325 [==============================] - 17s 579us/step - loss: 0.0770 - acc: 0.9756\n",
      "Epoch 89/200\n",
      "29325/29325 [==============================] - 17s 568us/step - loss: 0.0816 - acc: 0.9742\n",
      "Epoch 90/200\n",
      "29325/29325 [==============================] - 17s 572us/step - loss: 0.0756 - acc: 0.9751\n",
      "Epoch 91/200\n",
      "29325/29325 [==============================] - 17s 595us/step - loss: 0.0791 - acc: 0.9734\n",
      "Epoch 92/200\n",
      "29325/29325 [==============================] - 16s 551us/step - loss: 0.0730 - acc: 0.9761\n",
      "Epoch 93/200\n",
      "29325/29325 [==============================] - 16s 549us/step - loss: 0.0785 - acc: 0.9731\n",
      "Epoch 94/200\n",
      "29325/29325 [==============================] - 16s 549us/step - loss: 0.0695 - acc: 0.9777\n",
      "Epoch 95/200\n",
      "29325/29325 [==============================] - 16s 550us/step - loss: 0.0761 - acc: 0.9761\n",
      "Epoch 96/200\n",
      "29325/29325 [==============================] - 16s 549us/step - loss: 0.0740 - acc: 0.9764\n",
      "Epoch 97/200\n",
      "29325/29325 [==============================] - 16s 547us/step - loss: 0.0722 - acc: 0.9765\n",
      "Epoch 98/200\n",
      "29325/29325 [==============================] - 16s 546us/step - loss: 0.0717 - acc: 0.9771\n",
      "Epoch 99/200\n",
      "29325/29325 [==============================] - 16s 551us/step - loss: 0.0760 - acc: 0.9748\n",
      "Epoch 100/200\n",
      "29325/29325 [==============================] - 16s 551us/step - loss: 0.0747 - acc: 0.9758\n",
      "Epoch 101/200\n",
      "29325/29325 [==============================] - 17s 580us/step - loss: 0.0716 - acc: 0.9768\n",
      "Epoch 102/200\n",
      "29325/29325 [==============================] - 17s 569us/step - loss: 0.0804 - acc: 0.9740\n",
      "Epoch 103/200\n",
      "29325/29325 [==============================] - 16s 552us/step - loss: 0.0696 - acc: 0.9774\n",
      "Epoch 104/200\n",
      "29325/29325 [==============================] - 17s 564us/step - loss: 0.0671 - acc: 0.9787\n",
      "Epoch 105/200\n",
      "29325/29325 [==============================] - 16s 547us/step - loss: 0.0822 - acc: 0.9729\n",
      "Epoch 106/200\n",
      "29325/29325 [==============================] - 16s 556us/step - loss: 0.0840 - acc: 0.9727\n",
      "Epoch 107/200\n",
      "29325/29325 [==============================] - 16s 546us/step - loss: 0.0680 - acc: 0.9781\n",
      "Epoch 108/200\n",
      "29325/29325 [==============================] - 16s 558us/step - loss: 0.0685 - acc: 0.9776\n",
      "Epoch 109/200\n",
      "29325/29325 [==============================] - 17s 582us/step - loss: 0.0706 - acc: 0.9768\n",
      "Epoch 110/200\n",
      "29325/29325 [==============================] - 18s 611us/step - loss: 0.0727 - acc: 0.9768\n",
      "Epoch 111/200\n",
      "29325/29325 [==============================] - 17s 576us/step - loss: 0.0688 - acc: 0.9769\n",
      "Epoch 112/200\n",
      "29325/29325 [==============================] - 17s 590us/step - loss: 0.0738 - acc: 0.9761\n",
      "Epoch 113/200\n",
      "29325/29325 [==============================] - 17s 574us/step - loss: 0.0720 - acc: 0.9757\n",
      "Epoch 114/200\n",
      "29325/29325 [==============================] - 17s 569us/step - loss: 0.0705 - acc: 0.9774\n",
      "Epoch 115/200\n",
      "29325/29325 [==============================] - 17s 576us/step - loss: 0.0699 - acc: 0.9773\n",
      "Epoch 116/200\n",
      "29325/29325 [==============================] - 17s 566us/step - loss: 0.0821 - acc: 0.9739\n",
      "Epoch 117/200\n",
      "29325/29325 [==============================] - 17s 580us/step - loss: 0.0775 - acc: 0.9748\n",
      "Epoch 118/200\n",
      "29325/29325 [==============================] - 16s 560us/step - loss: 0.0638 - acc: 0.9783\n",
      "Epoch 119/200\n",
      "29325/29325 [==============================] - 16s 551us/step - loss: 0.0627 - acc: 0.9796\n",
      "Epoch 120/200\n",
      "29325/29325 [==============================] - 17s 564us/step - loss: 0.0551 - acc: 0.9821\n",
      "Epoch 121/200\n",
      "29325/29325 [==============================] - 17s 579us/step - loss: 0.0698 - acc: 0.9774\n",
      "Epoch 122/200\n",
      "29325/29325 [==============================] - 17s 585us/step - loss: 0.0733 - acc: 0.9764\n",
      "Epoch 123/200\n",
      "29325/29325 [==============================] - 17s 589us/step - loss: 0.0718 - acc: 0.9775\n",
      "Epoch 124/200\n",
      "29325/29325 [==============================] - 16s 551us/step - loss: 0.0646 - acc: 0.9783\n",
      "Epoch 125/200\n",
      "29325/29325 [==============================] - 17s 577us/step - loss: 0.0659 - acc: 0.9789\n",
      "Epoch 126/200\n",
      "29325/29325 [==============================] - 17s 585us/step - loss: 0.0771 - acc: 0.9752\n",
      "Epoch 127/200\n",
      "29325/29325 [==============================] - 18s 597us/step - loss: 0.0695 - acc: 0.9778\n",
      "Epoch 128/200\n",
      "29325/29325 [==============================] - 17s 589us/step - loss: 0.0658 - acc: 0.9786\n",
      "Epoch 129/200\n",
      "29325/29325 [==============================] - 17s 564us/step - loss: 0.0657 - acc: 0.9773\n",
      "Epoch 130/200\n",
      "29325/29325 [==============================] - 17s 568us/step - loss: 0.0689 - acc: 0.9769\n",
      "Epoch 131/200\n",
      "29325/29325 [==============================] - 16s 543us/step - loss: 0.0620 - acc: 0.9798\n",
      "Epoch 132/200\n",
      "29325/29325 [==============================] - 18s 606us/step - loss: 0.0709 - acc: 0.9764\n",
      "Epoch 133/200\n",
      "29325/29325 [==============================] - 16s 556us/step - loss: 0.0742 - acc: 0.9762\n",
      "Epoch 134/200\n",
      "29325/29325 [==============================] - 17s 566us/step - loss: 0.0676 - acc: 0.9774\n",
      "Epoch 135/200\n",
      "29325/29325 [==============================] - 16s 544us/step - loss: 0.0700 - acc: 0.9774\n",
      "Epoch 136/200\n",
      "29325/29325 [==============================] - 16s 545us/step - loss: 0.0664 - acc: 0.9775\n",
      "Epoch 137/200\n",
      "29325/29325 [==============================] - 16s 545us/step - loss: 0.0667 - acc: 0.9787\n",
      "Epoch 138/200\n",
      "29325/29325 [==============================] - 16s 548us/step - loss: 0.0660 - acc: 0.9793\n",
      "Epoch 139/200\n",
      "29325/29325 [==============================] - 17s 577us/step - loss: 0.0680 - acc: 0.9773\n",
      "Epoch 140/200\n",
      "29325/29325 [==============================] - 17s 568us/step - loss: 0.0670 - acc: 0.9784\n",
      "Epoch 141/200\n",
      "29325/29325 [==============================] - 17s 585us/step - loss: 0.0602 - acc: 0.9800\n",
      "Epoch 142/200\n",
      "29325/29325 [==============================] - 17s 572us/step - loss: 0.0644 - acc: 0.9798\n",
      "Epoch 143/200\n",
      "29325/29325 [==============================] - 17s 574us/step - loss: 0.0680 - acc: 0.9777\n",
      "Epoch 144/200\n",
      "29325/29325 [==============================] - 17s 578us/step - loss: 0.0761 - acc: 0.9761\n",
      "Epoch 145/200\n",
      "29325/29325 [==============================] - 18s 603us/step - loss: 0.0608 - acc: 0.9800\n",
      "Epoch 146/200\n",
      "29325/29325 [==============================] - 17s 570us/step - loss: 0.0566 - acc: 0.9818\n",
      "Epoch 147/200\n",
      "29325/29325 [==============================] - 16s 546us/step - loss: 0.0619 - acc: 0.9794\n",
      "Epoch 148/200\n",
      "29325/29325 [==============================] - 16s 547us/step - loss: 0.0684 - acc: 0.9787\n",
      "Epoch 149/200\n",
      "29325/29325 [==============================] - 17s 584us/step - loss: 0.0712 - acc: 0.9769\n",
      "Epoch 150/200\n",
      "29325/29325 [==============================] - 17s 578us/step - loss: 0.0594 - acc: 0.9803\n",
      "Epoch 151/200\n",
      "29325/29325 [==============================] - 16s 559us/step - loss: 0.0695 - acc: 0.9781\n",
      "Epoch 152/200\n",
      "29325/29325 [==============================] - 17s 564us/step - loss: 0.0626 - acc: 0.9795\n",
      "Epoch 153/200\n",
      "29325/29325 [==============================] - 17s 567us/step - loss: 0.0549 - acc: 0.9818\n",
      "Epoch 154/200\n",
      "29325/29325 [==============================] - 16s 559us/step - loss: 0.0632 - acc: 0.9797\n",
      "Epoch 155/200\n",
      "29325/29325 [==============================] - 17s 565us/step - loss: 0.0657 - acc: 0.9792\n",
      "Epoch 156/200\n",
      "29325/29325 [==============================] - 18s 606us/step - loss: 0.0618 - acc: 0.9794\n",
      "Epoch 157/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29325/29325 [==============================] - 17s 586us/step - loss: 0.0650 - acc: 0.9800\n",
      "Epoch 158/200\n",
      "29325/29325 [==============================] - 17s 564us/step - loss: 0.0601 - acc: 0.9808\n",
      "Epoch 159/200\n",
      "29325/29325 [==============================] - 16s 562us/step - loss: 0.0675 - acc: 0.9779\n",
      "Epoch 160/200\n",
      "29325/29325 [==============================] - 17s 564us/step - loss: 0.0649 - acc: 0.9787\n",
      "Epoch 161/200\n",
      "29325/29325 [==============================] - 16s 563us/step - loss: 0.0597 - acc: 0.9799\n",
      "Epoch 162/200\n",
      "29325/29325 [==============================] - 17s 568us/step - loss: 0.0574 - acc: 0.9809\n",
      "Epoch 163/200\n",
      "29325/29325 [==============================] - 17s 566us/step - loss: 0.0680 - acc: 0.9785\n",
      "Epoch 164/200\n",
      "29325/29325 [==============================] - 17s 566us/step - loss: 0.0593 - acc: 0.9806\n",
      "Epoch 165/200\n",
      "29325/29325 [==============================] - 17s 567us/step - loss: 0.0559 - acc: 0.9821\n",
      "Epoch 166/200\n",
      "29325/29325 [==============================] - 17s 563us/step - loss: 0.0709 - acc: 0.9775\n",
      "Epoch 167/200\n",
      "29325/29325 [==============================] - 17s 563us/step - loss: 0.0629 - acc: 0.9785\n",
      "Epoch 168/200\n",
      "29325/29325 [==============================] - 17s 569us/step - loss: 0.0618 - acc: 0.9807\n",
      "Epoch 169/200\n",
      "29325/29325 [==============================] - 17s 565us/step - loss: 0.0590 - acc: 0.9809\n",
      "Epoch 170/200\n",
      "29325/29325 [==============================] - 17s 564us/step - loss: 0.0499 - acc: 0.9846\n",
      "Epoch 171/200\n",
      "29325/29325 [==============================] - 17s 563us/step - loss: 0.0569 - acc: 0.9809\n",
      "Epoch 172/200\n",
      "29325/29325 [==============================] - 17s 565us/step - loss: 0.0650 - acc: 0.9793\n",
      "Epoch 173/200\n",
      "29325/29325 [==============================] - 17s 567us/step - loss: 0.0632 - acc: 0.9802\n",
      "Epoch 174/200\n",
      "29325/29325 [==============================] - 17s 565us/step - loss: 0.0656 - acc: 0.9793\n",
      "Epoch 175/200\n",
      "29325/29325 [==============================] - 17s 566us/step - loss: 0.0673 - acc: 0.9779\n",
      "Epoch 176/200\n",
      "29325/29325 [==============================] - 17s 569us/step - loss: 0.0592 - acc: 0.9810\n",
      "Epoch 177/200\n",
      "29325/29325 [==============================] - 17s 569us/step - loss: 0.0594 - acc: 0.9808\n",
      "Epoch 178/200\n",
      "29325/29325 [==============================] - 17s 566us/step - loss: 0.0608 - acc: 0.9806\n",
      "Epoch 179/200\n",
      "29325/29325 [==============================] - 17s 566us/step - loss: 0.0588 - acc: 0.9809\n",
      "Epoch 180/200\n",
      "29325/29325 [==============================] - 17s 565us/step - loss: 0.0613 - acc: 0.9793\n",
      "Epoch 181/200\n",
      "29325/29325 [==============================] - 17s 569us/step - loss: 0.0593 - acc: 0.9808\n",
      "Epoch 182/200\n",
      "29325/29325 [==============================] - 17s 571us/step - loss: 0.0584 - acc: 0.9817\n",
      "Epoch 183/200\n",
      "29325/29325 [==============================] - 17s 582us/step - loss: 0.0589 - acc: 0.9815\n",
      "Epoch 184/200\n",
      "29325/29325 [==============================] - 17s 569us/step - loss: 0.0608 - acc: 0.9793\n",
      "Epoch 185/200\n",
      "29325/29325 [==============================] - 17s 572us/step - loss: 0.0524 - acc: 0.9830\n",
      "Epoch 186/200\n",
      "29325/29325 [==============================] - 18s 602us/step - loss: 0.0598 - acc: 0.9810\n",
      "Epoch 187/200\n",
      "29325/29325 [==============================] - 18s 631us/step - loss: 0.0597 - acc: 0.9798\n",
      "Epoch 188/200\n",
      "29325/29325 [==============================] - 17s 581us/step - loss: 0.0595 - acc: 0.9810\n",
      "Epoch 189/200\n",
      "29325/29325 [==============================] - 18s 609us/step - loss: 0.0598 - acc: 0.9812\n",
      "Epoch 190/200\n",
      "29325/29325 [==============================] - 19s 640us/step - loss: 0.0606 - acc: 0.9804\n",
      "Epoch 191/200\n",
      "29325/29325 [==============================] - 16s 557us/step - loss: 0.0520 - acc: 0.9829\n",
      "Epoch 192/200\n",
      "29325/29325 [==============================] - 16s 551us/step - loss: 0.0500 - acc: 0.9825\n",
      "Epoch 193/200\n",
      "29325/29325 [==============================] - 16s 546us/step - loss: 0.0564 - acc: 0.9811\n",
      "Epoch 194/200\n",
      "29325/29325 [==============================] - 16s 545us/step - loss: 0.0498 - acc: 0.9833\n",
      "Epoch 195/200\n",
      "29325/29325 [==============================] - 16s 544us/step - loss: 0.0588 - acc: 0.9816\n",
      "Epoch 196/200\n",
      "29325/29325 [==============================] - 16s 546us/step - loss: 0.0675 - acc: 0.9787\n",
      "Epoch 197/200\n",
      "29325/29325 [==============================] - 16s 554us/step - loss: 0.0586 - acc: 0.9808\n",
      "Epoch 198/200\n",
      "29325/29325 [==============================] - 18s 601us/step - loss: 0.0549 - acc: 0.9823\n",
      "Epoch 199/200\n",
      "29325/29325 [==============================] - 17s 590us/step - loss: 0.0488 - acc: 0.9846\n",
      "Epoch 200/200\n",
      "29325/29325 [==============================] - 17s 577us/step - loss: 0.0548 - acc: 0.9825\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f601ff115c0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.fit(x, y, batch_size=64, epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nmeyer/Documents/cs155/project3/tensorflow/lib64/python3.6/site-packages/ipykernel_launcher.py:4: RuntimeWarning: divide by zero encountered in log\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "shall i compare thee to a summer's day?\n",
      "the rup-or my sure mine eyes be by your time\n",
      "and thering all bell with spire my from my lavimy they despice:\n",
      "the can thy nide rance the shand, would bave and rights will,\n",
      "that every wast to the worn whises bo thys,\n",
      "for thy life and beauty the love,\n",
      "that 's thy comproun the fleers words to thee,\n",
      "and do thy self or say the live and respect,\n",
      "then in encuest of do in the tife desing,\n",
      "when exeres not beauty let, and heart\n",
      "my hast this well wo blad mays thy self thy self,\n",
      "the 's ack you thy shate undethen what the beauty,\n",
      "with he sive and beauty thou lov's wit of yor niden,\n",
      "  ass as in tour sill thy self thy self thy self thy self thy self thy self thy self thy self thy self thy self thy self thy self thy self thy self thy self thy self thy self thy self thy self thy self thy self thy self thy self thy self thy self thy self thy self thy self thy self thy self thy self thy self thy self thy self thy self thy self thy self thy self thy self thy self thy self thy self thy self thy self thy self thy self thy self thy self thy self thy self thy self thy self thy self thy self thy self thy self thy self thy self thy self thy self thy self thy self thy self thy self thy \n"
     ]
    }
   ],
   "source": [
    "#seed = \"shall i compare thee to a summer's day?\\nthou art more lovely\"\n",
    "seed = \"shall i compare thee to a summer's day?\\n\"\n",
    "generated = \"\" + seed\n",
    "\n",
    "# Generates 14 lines of poem (More efficient)\n",
    "\n",
    "x_pred = np.zeros((1, char_length - 1, len(chars)))\n",
    "for t, char in enumerate(seed):\n",
    "    x_pred[0, t, char_indices[char]] = 1.\n",
    "preds = model3.predict(x_pred, verbose=0)[0]\n",
    "\n",
    "max_in_line = 300\n",
    "\n",
    "for i in range(14):\n",
    "    print(i)\n",
    "    count = 0\n",
    "    while True:\n",
    "        count += 1\n",
    "        next_index = sample(preds, temperature=0.5)\n",
    "        next_char = indices_char[next_index]\n",
    "        \n",
    "        x_pred[0, :-1, :] = x_pred[:, 1:, :]\n",
    "        x_pred[0, -1, :] = np.zeros(len(chars))\n",
    "        x_pred[0, -1, char_indices[next_char]] = 1.\n",
    "        preds = model3.predict(x_pred, verbose=0)[0]\n",
    "        \n",
    "        generated += next_char\n",
    "        # sometimes the RNN can get stuck in an infinite loop of predictions\n",
    "        if next_char == \"\\n\" or count > max_in_line:\n",
    "            #print(generated)\n",
    "            #generated = \"\"\n",
    "            break\n",
    "        #generated += next_char\n",
    "        \n",
    "\n",
    "print(generated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you need to install h5py for this to work\n",
    "model1_save = keras.models.clone_model(model)\n",
    "model1_save.set_weights(model.get_weights())\n",
    "model1_save.save(\"RNN_1_dropout0.3.h5\")\n",
    "\n",
    "model2_save = keras.models.clone_model(model2)\n",
    "model2_save.set_weights(model2.get_weights())\n",
    "model2_save.save(\"RNN_2_dropout0.1.h5\")\n",
    "\n",
    "model3_save = keras.models.clone_model(model3)\n",
    "model3_save.set_weights(model3.get_weights())\n",
    "model3.save(\"RNN_3_dropout0.5.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following can be ignored (because I installed h5py in the virtualenv after running all the training, a variable in a submodule was not defined because the package checked for h5py to decide whether or not to set the variable. Everything after this is just to set that variable so that this work wouldn't be lost)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tensorflow.python.keras._impl.keras.engine',\n",
       " 'tensorflow.python.keras._impl.keras.engine.base_layer',\n",
       " 'tensorflow.python.keras._impl.keras.engine.input_layer',\n",
       " 'tensorflow.python.keras._impl.keras.engine.network',\n",
       " 'tensorflow.python.keras._impl.keras.engine.saving',\n",
       " 'tensorflow.python.keras._impl.keras.engine.training',\n",
       " 'tensorflow.python.keras._impl.keras.engine.training_arrays',\n",
       " 'tensorflow.python.keras._impl.keras.engine.training_utils',\n",
       " 'tensorflow.python.keras._impl.keras.engine.training_eager',\n",
       " 'tensorflow.python.keras._impl.keras.engine.training_generator',\n",
       " 'tensorflow.python.keras._impl.keras.engine.sequential',\n",
       " 'pandas.core.computation.engines',\n",
       " 'keras.engine',\n",
       " 'keras.engine.topology',\n",
       " 'keras.engine.training']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "[k for k in sys.modules.keys() if \"engine\" in k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.engine.topology.HDF5_OBJECT_HEADER_LIMIT = 64512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
